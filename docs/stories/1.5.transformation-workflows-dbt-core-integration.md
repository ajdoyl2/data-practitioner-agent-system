# Story 1.4: Transformation Workflows - dbt-core Integration

## Status
Draft

## Story
**As a** data engineer,
**I want** to define and execute transformation workflows using dbt-core,
**so that** I can maintain data quality and lineage.

## Acceptance Criteria
1. Integrate dbt-core with Python subprocess execution
2. Create guided ELT modeling templates (elt-modeling-guidance.md)
3. Implement dbt project initialization within expansion pack
4. Configure testing patterns (generic and custom tests)
5. Enable documentation generation from dbt models
6. Integrate API key authentication for dbt transformation endpoints
7. Implement feature flag checking for dbt_transformations feature
8. Add security logging for all dbt transformation operations

## Integration Verification
- IV1: Existing validation scripts continue functioning
- IV2: dbt operations don't interfere with BMad build processes
- IV3: Test execution times remain reasonable (<5 minutes for standard suite)
- IV4: Security framework from Story 1.1.5 properly integrated
- IV5: API key authentication validates properly for dbt operations
- IV6: Feature flag 'dbt_transformations' controls service availability
- IV7: Security events logged for all transformation operations

## Tasks / Subtasks

- [ ] Task 1: Install and configure dbt-core environment (AC: 1)
  - [ ] Add dbt-core ^1.8.8 to requirements.txt with DuckDB adapter
  - [ ] Create Python subprocess wrapper for dbt command execution
  - [ ] Configure dbt project structure within expansion pack
  - [ ] Create dbt profiles.yml configuration for DuckDB connection
  - [ ] Implement dbt command execution with proper error handling
  - [ ] Test dbt installation and basic model compilation

- [ ] Task 2: Create TransformationEngine component (AC: 1, 3, 6, 7, 8) [DEPENDS ON Story 1.1.5]
  - [ ] Create `tools/data-services/transformation-engine.js`
  - [ ] Implement dbt-core project initialization and model execution
  - [ ] Add dbt subprocess execution with JSON communication
  - [ ] Create dbt command wrapper functions (run, test, docs, compile)
  - [ ] Implement model dependency resolution and execution ordering
  - [ ] Add integration with existing task execution patterns
  - [ ] Integrate authentication middleware for transformation endpoints
  - [ ] Add feature flag checking for 'dbt_transformations' before operations
  - [ ] Implement security logging for all transformation operations

- [ ] Task 3: Implement dbt project initialization (AC: 3)
  - [ ] Create automated dbt project setup within bmad-data-practitioner
  - [ ] Generate dbt_project.yml with proper configuration
  - [ ] Create standard directory structure (models/, tests/, macros/, docs/)
  - [ ] Add DuckDB-specific dbt adapter configuration
  - [ ] Implement project validation and setup verification
  - [ ] Create project templates for different analysis types

- [ ] Task 4: Create guided ELT modeling templates (AC: 2)
  - [ ] Create `bmad-data-practitioner/tasks/elt-modeling-guidance.md`
  - [ ] Design layered architecture templates (Source → Staging → Intermediate → Marts)
  - [ ] Add template workflows for common transformation patterns
  - [ ] Create guided workflows following BMad elicitation patterns
  - [ ] Implement model generation helpers for standard patterns
  - [ ] Add model naming conventions and best practices guidance

- [ ] Task 5: Configure testing patterns (AC: 4)
  - [ ] Implement generic dbt tests (unique, not_null, accepted_values, relationships)
  - [ ] Create custom SQL-based test templates
  - [ ] Add data quality validation framework
  - [ ] Configure automated test generation based on model schemas
  - [ ] Implement test result parsing and reporting
  - [ ] Add integration with existing BMad validation systems

- [ ] Task 6: Enable documentation and lineage generation (AC: 5)
  - [ ] Configure dbt docs generation with custom styling
  - [ ] Implement lineage visualization integration
  - [ ] Create automated documentation updates
  - [ ] Add model description templates and standards
  - [ ] Integrate documentation with BMad documentation patterns
  - [ ] Configure docs hosting and access through BMad web-builder

- [ ] Task 7: Data transformation workflow integration
  - [ ] Create interface between AnalyticalEngine and TransformationEngine
  - [ ] Implement DuckDB source table registration for dbt models
  - [ ] Add incremental model support for large datasets
  - [ ] Create transformation job scheduling and monitoring
  - [ ] Implement transformation result validation and quality checks
  - [ ] Add transformation performance monitoring and optimization

- [ ] Task 8: Integration testing and validation (All IVs)
  - [ ] Verify existing validation scripts continue functioning (IV1)
  - [ ] Test dbt operations isolation from BMad build processes (IV2)
  - [ ] Monitor test execution times for performance compliance (IV3)
  - [ ] Run regression tests on existing BMad functionality
  - [ ] Performance testing for transformation workflows

## Dev Notes

### Previous Story Context
Stories 1.2 and 1.3 implemented data ingestion (PyAirbyte) and analytical processing (DuckDB). Story 1.1.5 implemented the security and risk management foundation with API key authentication, feature flags, and security logging. This story builds the transformation layer between raw ingested data and analytics-ready datasets using dbt-core for maintaining data quality and lineage while integrating security controls.

### TransformationEngine Architecture
[Source: architecture/component-architecture.md#transformationengine]

**Responsibility:** Executes dbt-core transformation workflows, manages data lineage tracking, and maintains data quality through comprehensive testing

**Key Interfaces Required:**
- dbt-core project initialization and model execution
- Guided ELT modeling templates following BMad elicitation patterns
- Data quality testing framework with automated test generation
- Lineage visualization and documentation generation

**Technology Stack Dependencies:**
- dbt-core ^1.8.8 (data transformation framework)
- Python subprocess execution for dbt command processing
- YAML-based configuration following BMad patterns
- Existing Components: Task execution framework, template engine, validation systems
- New Components: AnalyticalEngine (data sources - Story 1.3), WorkflowOrchestrator (pipeline coordination - Story 1.5)

### dbt-core Technical Specifications
[Source: architecture/tech-stack-alignment.md#new-technology-additions]

**dbt-core Version:** ^1.8.8  
**Purpose:** Data transformation workflows  
**Rationale:** Latest stable with improved Jinja rendering and enhanced testing framework  
**Integration Method:** Python subprocess execution

**Key Capabilities to Implement:**
- Layered architecture approach (Source → Staging → Intermediate → Marts)
- Generic tests (unique, not_null, accepted_values, relationships) and custom SQL-based assertions
- CI/CD integration patterns with selective execution for modified models
- Comprehensive testing patterns and documentation generation

### DataTransformation Model Integration
[Source: architecture/data-models-and-schema-changes.md#datatransformation]

**DataTransformation Model Structure:**
- transformation_id: String - Unique identifier for dbt model tracking
- model_name: String - dbt model name following naming conventions
- source_references: Array[String] - References to DataSource entities from Stories 1.2/1.3
- transformation_logic: String - SQL transformation logic (stored in dbt models)
- test_results: Object - dbt test execution results and quality metrics
- lineage_metadata: Object - Upstream and downstream dependencies
- execution_metadata: Object - Performance and resource usage tracking

**Integration Relationships:**
- With Existing: Follows existing task workflow patterns for execution tracking
- With New: Child of AnalysisProject, feeds into InsightDocument generation (Story 1.7)

### File Structure Requirements
[Source: architecture/source-tree-integration.md#new-file-organization]

**New Files to Create:**
```plaintext
expansion-packs/bmad-data-practitioner/
├── dbt-project/                    # NEW: dbt project structure
│   ├── dbt_project.yml
│   ├── profiles.yml
│   ├── models/
│   │   ├── staging/
│   │   ├── intermediate/
│   │   └── marts/
│   ├── tests/
│   ├── macros/
│   └── docs/
tools/data-services/
├── transformation-engine.js        # NEW: dbt operations and workflow management
├── dbt-wrapper.js                 # NEW: dbt subprocess execution wrapper
└── model-generator.js             # NEW: Model template generation helpers
```

### dbt Project Configuration
**DuckDB Integration Configuration:**
```yaml
# dbt_project.yml
name: 'bmad_data_practitioner'
version: '1.0.0'
config-version: 2

model-paths: ["models"]
analysis-paths: ["analysis"]
test-paths: ["tests"]
seed-paths: ["data"]
macro-paths: ["macros"]

target-path: "target"
clean-targets: ["target", "dbt_packages"]

# Security configuration
vars:
  security:
    require_authentication: true
    feature_flag: 'dbt_transformations'
    log_security_events: true

models:
  bmad_data_practitioner:
    staging:
      +materialized: view
    intermediate:
      +materialized: view
    marts:
      +materialized: table

# profiles.yml
bmad_data_practitioner:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: '../../../.duckdb/analytics.db'
      threads: 4
```

### Layered Architecture Implementation
**ELT Modeling Structure:**
- **Source Layer**: Raw data from PyAirbyte ingestion, registered as dbt sources
- **Staging Layer**: One-to-one with source tables, basic cleansing and type casting
- **Intermediate Layer**: Business logic transformations, joins, and aggregations
- **Marts Layer**: Final analytics-ready datasets for consumption

**Model Naming Conventions:**
- Staging: `stg_[source_name]__[table_name]`
- Intermediate: `int_[business_concept]__[description]`
- Marts: `[business_area]__[entity_type]` (e.g., `sales__customer_metrics`)

### Testing Framework Implementation
**Generic Test Configuration:**
- unique: Ensure column values are unique
- not_null: Ensure column values are not null
- accepted_values: Validate column values against allowed list
- relationships: Validate foreign key relationships

**Custom Test Patterns:**
- Data quality thresholds (completeness, accuracy, consistency)
- Business rule validation (e.g., positive revenue amounts)
- Statistical outlier detection
- Cross-table consistency checks

### Python Integration Standards
[Source: architecture/coding-standards-and-conventions.md#enhancement-specific-standards]

**Python Code Integration Standards:**
- All dbt commands executed through Node.js subprocess interfaces
- dbt project follows standard conventions with DuckDB adapter
- Virtual environment isolation with pinned dependency versions
- Error handling with proper subprocess management

**dbt Command Execution Patterns:**
```javascript
// Example dbt subprocess execution pattern
const dbtRun = async (models = null, fullRefresh = false) => {
  const args = ['run'];
  if (models) args.push('--models', models);
  if (fullRefresh) args.push('--full-refresh');
  
  return await executeDbtCommand(args);
};
```

### Integration with BMad Template System
[Source: architecture/component-architecture.md#transformationengine]

**Guided ELT Modeling Templates:**
- Integration with BMad template system for guided ELT workflows
- Elicitation patterns for model generation and configuration
- Template-driven model creation with validation
- Interactive workflows for transformation design

### Performance and Quality Requirements
**Data Quality Framework:**
- Automated test generation based on model schemas and business rules
- Comprehensive testing patterns with 80% minimum coverage
- Test result parsing and integration with BMad validation systems
- Performance monitoring for transformation execution times

**Transformation Optimization:**
- Incremental model support for large datasets
- Query optimization recommendations based on DuckDB capabilities
- Resource usage monitoring and optimization suggestions
- Selective model execution for development efficiency

### Error Handling and Process Management
[Source: architecture/coding-standards-and-conventions.md#enhancement-specific-standards]

**dbt Operations Error Handling:**
- Python subprocess errors wrapped and translated to Node.js error objects
- Comprehensive error handling for dbt compilation, testing, and execution failures
- Graceful degradation when dbt tools unavailable with clear user messaging
- Process timeout handling for long-running transformations

### Integration Points with Existing System
[Source: architecture/coding-standards-and-conventions.md#critical-integration-rules]

**Existing API Compatibility:** All existing BMad-Method validation scripts and build processes must remain completely unchanged - dbt operations are isolated within the data processing pipeline

**BMad Integration Requirements:**
- Follow existing task execution patterns for transformation workflows
- Integrate with template engine for guided modeling workflows
- Use existing validation systems for transformation quality gates
- Maintain existing progress tracking patterns (ora, chalk)

## Testing

### Testing Standards from Architecture
[Source: architecture/testing-strategy.md#new-testing-requirements]

**Framework:** Jest ^30.0.4 extended with dbt testing utilities and subprocess validation  
**Test Location:** `/tests/data-services/` following existing test organization patterns  
**Coverage Target:** 80% minimum coverage for all data processing components

**Specific Testing Requirements for This Story:**
- dbt subprocess execution and command wrapper testing
- Model compilation and execution validation
- Data quality testing framework verification
- Transformation lineage and documentation generation testing
- Performance testing for transformation execution times
- Integration testing with AnalyticalEngine (DuckDB) from Story 1.3

**Test Files to Create:**
- `/tests/data-services/transformation-engine.test.js` - Core dbt operations testing
- `/tests/data-services/dbt-wrapper.test.js` - dbt subprocess wrapper functionality
- `/tests/data-services/model-generator.test.js` - Model template generation testing
- `/tests/data-services/data-quality-testing.test.js` - Testing framework validation
- `/tests/integration/duckdb-dbt-pipeline.test.js` - End-to-end transformation pipeline
- `/tests/performance/transformation-execution-time.test.js` - Performance compliance validation
- `/tests/security/dbt-authentication.test.js` - API key authentication for dbt endpoints
- `/tests/security/dbt-feature-flags.test.js` - Feature flag validation for dbt operations
- `/tests/security/dbt-security-logging.test.js` - Security event logging validation

**Integration Testing Scope:**
[Source: architecture/testing-strategy.md#integration-tests]
- Complete data transformation pipeline testing (DuckDB → dbt → transformed datasets)
- Cross-component data flow validation with AnalyticalEngine
- Existing System Verification: All existing BMad-Method functionality must continue working unchanged

**Critical Test Scenarios:**
- dbt project initialization and configuration validation
- Model compilation and execution across all layers (staging, intermediate, marts)
- Data quality testing with various test patterns and custom validations
- Documentation generation and lineage visualization
- Incremental model execution with large datasets
- Error handling for dbt compilation and execution failures
- Performance compliance for test execution times (<5 minutes)
- Integration with existing BMad validation scripts
- dbt operations isolation from BMad build processes

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-08 | 1.0 | Initial story creation for dbt-core transformation workflows | SM Bob |
| 2025-08-09 | 1.1 | Added security integration requirements from Story 1.1.5 | Security Architect Winston |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*