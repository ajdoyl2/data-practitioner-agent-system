# Story 1.4: Workflow Orchestration - Dagster Integration

## Status
Draft

## Story
**As a** data architect,
**I want** to orchestrate data pipelines using Dagster's asset-centric approach,
**so that** I can manage complex data workflows with proper dependencies.

## Acceptance Criteria
1. Implement Dagster integration with configuration management
2. Create asset definitions for data pipeline components
3. Configure scheduling and triggering mechanisms
4. Implement monitoring and alerting interfaces
5. Enable lineage tracking and visualization

## Integration Verification
- IV1: Existing workflow patterns in BMad-Method remain functional
- IV2: Dagster UI accessible without conflicting with other tools
- IV3: Resource usage remains manageable with Dagster daemon running

## Tasks / Subtasks

- [ ] Task 1: Install and configure Dagster environment (AC: 1)
  - [ ] Add Dagster ^1.8.12 to requirements.txt with required dependencies
  - [ ] Create Python subprocess wrapper for Dagster daemon and web UI
  - [ ] Configure Dagster workspace and project structure
  - [ ] Create Dagster configuration files (dagster.yaml, workspace.yaml)
  - [ ] Implement Dagster daemon process management with proper lifecycle
  - [ ] Test Dagster installation and basic asset execution

- [ ] Task 2: Create WorkflowOrchestrator component (AC: 1, 2)
  - [ ] Create `tools/data-services/workflow-orchestrator.js`
  - [ ] Implement Dagster asset definition and dependency management
  - [ ] Add Python subprocess execution for Dagster commands
  - [ ] Create asset registration interface for data pipeline components
  - [ ] Implement dependency resolution and execution ordering
  - [ ] Add integration with existing BMad workflow patterns

- [ ] Task 3: Define assets for existing data pipeline components (AC: 2)
  - [ ] Create Dagster assets for PyAirbyte data ingestion (Story 1.2 integration)
  - [ ] Define DuckDB analytical processing assets (Story 1.3 integration)
  - [ ] Create dbt transformation workflow assets (Story 1.5 integration)
  - [ ] Implement asset dependency mapping across the complete pipeline
  - [ ] Add asset metadata and documentation for lineage tracking
  - [ ] Configure asset partitioning for scalable data processing

- [ ] Task 4: Configure scheduling and triggering (AC: 3)
  - [ ] Implement time-based scheduling for periodic data refreshes
  - [ ] Create event-driven triggers for data source changes
  - [ ] Add manual trigger mechanisms through BMad agent interfaces
  - [ ] Configure retry policies and failure handling strategies
  - [ ] Implement conditional execution based on data quality checks
  - [ ] Add scheduling integration with existing BMad task patterns

- [ ] Task 5: Implement monitoring and alerting (AC: 4)
  - [ ] Create monitoring dashboard integration with BMad progress tracking
  - [ ] Implement alerting for pipeline failures and data quality issues
  - [ ] Add resource usage monitoring and optimization recommendations
  - [ ] Create performance metrics collection and reporting
  - [ ] Implement notification channels (email, webhook, BMad UI)
  - [ ] Add integration with existing BMad error handling patterns

- [ ] Task 6: Enable lineage tracking and visualization (AC: 5)
  - [ ] Configure Dagster lineage visualization for complete data pipeline
  - [ ] Create asset lineage documentation and metadata management
  - [ ] Implement data flow tracking from ingestion to publication
  - [ ] Add lineage integration with dbt documentation from Story 1.5
  - [ ] Create lineage export capabilities for external documentation
  - [ ] Add impact analysis for data pipeline changes

- [ ] Task 7: Dagster UI and web interface integration
  - [ ] Configure Dagster web UI with proper authentication
  - [ ] Integrate Dagster UI with existing BMad web-builder patterns
  - [ ] Create custom Dagster UI extensions for BMad-specific workflows
  - [ ] Add asset run history and debugging interfaces
  - [ ] Implement asset catalog browsing and search functionality
  - [ ] Configure UI access controls and user management

- [ ] Task 8: Integration testing and validation (All IVs)
  - [ ] Verify existing BMad workflow patterns remain functional (IV1)
  - [ ] Test Dagster UI accessibility without tool conflicts (IV2)
  - [ ] Monitor resource usage with Dagster daemon running (IV3)
  - [ ] Run regression tests on existing BMad functionality
  - [ ] Performance testing for orchestrated pipeline execution

## Dev Notes

### Previous Story Context
Stories 1.1-1.3 implemented the foundational data pipeline components: agent infrastructure, PyAirbyte ingestion, and DuckDB analytics. This story adds sophisticated orchestration and monitoring capabilities that will coordinate these components and the upcoming dbt transformations (Story 1.5) as unified workflows.

### WorkflowOrchestrator Architecture
[Source: architecture/component-architecture.md#workfloworchestrator]

**Responsibility:** Manages Dagster asset-centric workflow orchestration, pipeline scheduling, and comprehensive monitoring across all data operations

**Key Interfaces Required:**
- Dagster asset definition and dependency management
- Pipeline scheduling and trigger mechanism configuration
- Monitoring dashboard integration with existing BMad progress tracking
- Error handling and retry logic following BMad failure recovery patterns

**Technology Stack Dependencies:**
- Dagster ^1.8.12 (workflow orchestration framework)
- Python subprocess execution for Dagster daemon and web UI
- Web UI integration for monitoring and management
- Existing Components: CLI orchestration, progress tracking (ora), workflow management systems
- New Components: DataIngestionService (Story 1.2), AnalyticalEngine (Story 1.3), TransformationEngine (Story 1.5), PublicationEngine (Story 1.7)

### Dagster Technical Specifications
[Source: architecture/tech-stack-alignment.md#new-technology-additions]

**Dagster Version:** ^1.8.12  
**Purpose:** Workflow orchestration  
**Rationale:** Latest stable with enhanced asset lineage and improved web UI performance  
**Integration Method:** Python subprocess with web UI integration

**Key Capabilities to Implement:**
- Asset-centric design treating data assets (tables, files, ML models) as first-class citizens
- Software-defined assets with intuitive dependency tracking and built-in lineage visualization
- Asset checks and data quality monitoring with real-time alerts
- Partitioning strategies (time-based, custom, dynamic) with resource management and cost optimization

### Asset-Centric Design Implementation
**Asset Definition Strategy:**
- Data Sources: PyAirbyte connectors as upstream assets with refresh scheduling
- Analytical Tables: DuckDB tables as materialized assets with dependency tracking
- Transformation Models: dbt models as assets with lineage to source tables
- Publication Artifacts: Evidence.dev sites as downstream assets consuming transformed data

**Asset Dependency Mapping:**
```python
# Example asset dependency structure
@asset(deps=[pyairbyte_data_source])
def duckdb_analytical_table():
    # Load data from PyAirbyte into DuckDB
    pass

@asset(deps=[duckdb_analytical_table])
def dbt_transformation_model():
    # Execute dbt transformations
    pass

@asset(deps=[dbt_transformation_model])
def evidence_publication():
    # Generate Evidence.dev publication
    pass
```

### File Structure Requirements
[Source: architecture/source-tree-integration.md#new-file-organization]

**New Files to Create:**
```plaintext
expansion-packs/bmad-data-practitioner/
├── dagster-project/                # NEW: Dagster workspace structure
│   ├── dagster.yaml
│   ├── workspace.yaml
│   ├── assets/
│   │   ├── ingestion_assets.py
│   │   ├── analytics_assets.py
│   │   ├── transformation_assets.py
│   │   └── publication_assets.py
│   ├── jobs/
│   ├── schedules/
│   ├── sensors/
│   └── resources/
tools/data-services/
├── workflow-orchestrator.js        # NEW: Dagster operations and coordination
├── dagster-wrapper.js             # NEW: Dagster subprocess execution wrapper
├── asset-manager.js               # NEW: Asset definition and dependency management
└── pipeline-monitor.js            # NEW: Monitoring and alerting functionality
```

### Dagster Configuration Management
**Dagster Workspace Configuration:**
```yaml
# dagster.yaml
scheduler:
  module: dagster.core.scheduler
  class: DagsterDaemonScheduler

run_coordinator:
  module: dagster.core.run_coordinator
  class: DefaultRunCoordinator

compute_logs:
  module: dagster.core.storage.compute_log_manager
  class: LocalComputeLogManager
  config:
    base_dir: "logs"

# workspace.yaml
load_from:
  - python_file: assets/ingestion_assets.py
  - python_file: assets/analytics_assets.py
  - python_file: assets/transformation_assets.py
  - python_file: assets/publication_assets.py
```

### Monitoring and Alerting Framework
**Integration with BMad Progress Tracking:**
- Use existing BMad progress patterns (ora, chalk) for pipeline status display
- Extend existing error handling patterns with Dagster-specific error contexts
- Integrate with existing CLI orchestration for manual pipeline triggers
- Add pipeline status to existing BMad workflow management systems

**Alerting Configuration:**
```yaml
monitoring:
  dagster:
    webUI:
      enabled: true
      port: 3001
      host: "localhost"
    alerting:
      channels:
        - type: "webhook"
          url: "http://localhost:3000/api/v1/alerts"
        - type: "console"
          level: "error"
    metrics:
      collection_interval: "30s"
      retention_period: "7d"
```

### Scheduling and Triggering Strategies
**Time-Based Scheduling:**
- Daily data refresh schedules for regular reporting
- Hourly incremental updates for real-time analytics
- Weekly comprehensive pipeline execution for data quality validation

**Event-Driven Triggers:**
- File system sensors for new data source detection
- Database change sensors for upstream data modifications
- API webhook sensors for external system notifications

**Manual Trigger Integration:**
- BMad agent command integration for on-demand pipeline execution
- CLI trigger mechanisms following existing BMad patterns
- Web UI integration for manual pipeline management

### Performance and Resource Management
**Resource Usage Optimization:**
- Configurable resource limits for Dagster daemon and web UI processes
- Asset partitioning strategies for efficient processing of large datasets
- Memory usage monitoring with automatic cleanup of completed runs
- Concurrent execution limits to prevent system overload

**Performance Monitoring:**
- Asset execution time tracking with performance regression detection
- Resource utilization monitoring (CPU, memory, disk I/O)
- Pipeline bottleneck identification and optimization recommendations
- Cost optimization through efficient scheduling and resource allocation

### Integration with Previous Stories
**PyAirbyte Integration (Story 1.2):**
- Wrap PyAirbyte data ingestion processes as Dagster assets
- Add scheduling for periodic data source refresh
- Implement data source monitoring and failure alerting

**DuckDB Integration (Story 1.3):**
- Define DuckDB analytical operations as Dagster assets
- Add dependency tracking between ingestion and analytics assets
- Implement analytical query scheduling and monitoring

**dbt Integration (Story 1.5):**
- Integrate dbt transformation workflows as Dagster assets
- Add lineage tracking between source data and transformation outputs
- Implement transformation quality monitoring and validation

### Error Handling and Recovery Patterns
[Source: architecture/coding-standards-and-conventions.md#enhancement-specific-standards]

**Dagster Error Handling:**
- Python subprocess errors wrapped and translated to Node.js error objects
- Comprehensive retry policies for transient failures
- Graceful degradation when Dagster daemon unavailable
- Asset failure isolation to prevent cascade failures
- Recovery workflows for partial pipeline failures

### Integration Points with Existing System
[Source: architecture/coding-standards-and-conventions.md#critical-integration-rules]

**Existing API Compatibility:** All existing BMad-Method workflow patterns must remain completely unchanged - Dagster orchestration operates as an additional layer over existing functionality

**BMad Integration Requirements:**
- Extend existing BMad workflow patterns with Dagster coordination
- Integrate with CLI orchestration for pipeline management commands
- Use existing progress tracking patterns for pipeline status display
- Maintain existing error handling and recovery mechanisms

## Testing

### Testing Standards from Architecture
[Source: architecture/testing-strategy.md#new-testing-requirements]

**Framework:** Jest ^30.0.4 extended with Dagster testing utilities and subprocess validation  
**Test Location:** `/tests/data-services/` following existing test organization patterns  
**Coverage Target:** 80% minimum coverage for all data processing components

**Specific Testing Requirements for This Story:**
- Dagster daemon and web UI subprocess management testing
- Asset definition and dependency resolution validation
- Pipeline scheduling and execution testing
- Monitoring and alerting functionality verification
- Performance testing for orchestrated pipeline execution
- Integration testing with all previous story components (1.2-1.3)

**Test Files to Create:**
- `/tests/data-services/workflow-orchestrator.test.js` - Core Dagster operations testing
- `/tests/data-services/dagster-wrapper.test.js` - Dagster subprocess wrapper functionality
- `/tests/data-services/asset-manager.test.js` - Asset definition and dependency management
- `/tests/data-services/pipeline-monitor.test.js` - Monitoring and alerting validation
- `/tests/integration/complete-data-pipeline.test.js` - End-to-end pipeline orchestration
- `/tests/performance/pipeline-orchestration.test.js` - Performance and resource usage validation

**Integration Testing Scope:**
[Source: architecture/testing-strategy.md#integration-tests]
- Complete data pipeline orchestration (PyAirbyte → DuckDB → dbt → monitoring)
- Cross-component asset dependency validation
- Existing System Verification: All existing BMad-Method functionality must continue working unchanged

**Critical Test Scenarios:**
- Dagster daemon startup and shutdown with proper resource cleanup
- Asset execution with dependency resolution across all pipeline components
- Pipeline scheduling and triggering mechanisms
- Error handling and recovery for asset failures
- Resource usage monitoring with Dagster daemon running
- Web UI accessibility without conflicts with existing tools
- Performance compliance for orchestrated vs. individual component execution
- Integration with existing BMad workflow patterns and CLI commands

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-08 | 1.0 | Initial story creation for Dagster workflow orchestration | SM Bob |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*