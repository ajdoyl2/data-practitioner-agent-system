# Story 1.2: Data Ingestion - PyAirbyte Integration

## Status
Draft

## Story
**As a** data engineer,
**I want** to ingest data from multiple sources through PyAirbyte connectors,
**so that** I can work with diverse datasets in the BMad-Method framework.

## Acceptance Criteria
1. Implement Node.js wrapper for PyAirbyte Python execution
2. Create data ingestion API endpoints for file upload and database connections
3. Configure PyAirbyte connectors for common sources (CSV, JSON, databases)
4. Implement stream selection and cache management capabilities
5. Create ingestion task templates (data-source-discovery.md)

## Integration Verification
- IV1: Existing CLI tools remain functional with new Python dependencies
- IV2: PyAirbyte subprocess execution doesn't block Node.js event loop
- IV3: Memory usage remains within acceptable limits during data ingestion

## Tasks / Subtasks

- [ ] Task 1: Implement Python environment and PyAirbyte wrapper (AC: 1)
  - [ ] Create Python virtual environment setup in installer
  - [ ] Add PyAirbyte ^0.20.0 to requirements.txt
  - [ ] Create Node.js subprocess wrapper for PyAirbyte execution
  - [ ] Implement JSON communication protocol between Node.js and Python
  - [ ] Add error handling and process management for Python subprocess
  - [ ] Test PyAirbyte installation and basic connectivity

- [ ] Task 2: Create DataIngestionService component (AC: 1, 2)
  - [ ] Create `tools/data-services/data-ingestion-service.js`
  - [ ] Implement REST API endpoints under `/api/v1/data-sources`
  - [ ] Add file upload handling for CSV, JSON, and other formats
  - [ ] Create database connection configuration interface
  - [ ] Integrate with existing commander.js CLI patterns
  - [ ] Add YAML configuration management following technical-preferences patterns

- [ ] Task 3: Configure PyAirbyte connectors (AC: 3)
  - [ ] Set up CSV file connector with configurable options
  - [ ] Set up JSON file connector with nested data support
  - [ ] Configure database connectors (PostgreSQL, MySQL, SQLite)
  - [ ] Implement connector discovery and validation
  - [ ] Create connector configuration templates
  - [ ] Test connector initialization and basic data extraction

- [ ] Task 4: Implement stream selection and caching (AC: 4)
  - [ ] Add stream discovery functionality through PyAirbyte
  - [ ] Implement stream selection UI/API for data source configuration
  - [ ] Create cache management system for PyAirbyte extracted data
  - [ ] Implement cache invalidation and refresh mechanisms
  - [ ] Add cache location configuration in core-config.yaml
  - [ ] Test caching performance with various data sizes

- [ ] Task 5: Create ingestion task templates (AC: 5)
  - [ ] Create `bmad-data-practitioner/tasks/data-source-discovery.md`
  - [ ] Add template for data source configuration workflows
  - [ ] Create guided workflows for common ingestion scenarios
  - [ ] Integrate templates with existing BMad elicitation patterns
  - [ ] Test template rendering and agent execution

- [ ] Task 6: Integration testing and validation (All IVs)
  - [ ] Test existing CLI tools continue functioning (IV1)
  - [ ] Validate PyAirbyte subprocess doesn't block Node.js (IV2)
  - [ ] Monitor memory usage during large data ingestion (IV3)
  - [ ] Run regression tests on existing BMad functionality
  - [ ] Performance testing for data ingestion workflows

## Dev Notes

### Previous Story Context
Story 1.1 established the expansion pack directory structure and agent definitions. This story builds on that foundation by implementing the first data processing component - the DataIngestionService.

### DataIngestionService Architecture
[Source: architecture/component-architecture.md#dataingestionservice]

**Responsibility:** Manages PyAirbyte connector execution, data source discovery, and initial data validation for diverse input sources

**Key Interfaces Required:**
- REST API endpoints for file upload and database connection configuration
- PyAirbyte subprocess execution with JSON communication protocols
- DuckDB connection interface for data loading and validation (interface only, DuckDB implementation in Story 1.3)
- YAML configuration interface following existing technical-preferences patterns

**Technology Stack Dependencies:**
- Node.js with Python subprocess execution
- PyAirbyte ^0.20.0 (Python package)
- Connection pooling for database sources
- Existing Components: CLI framework (commander), file system utilities (fs-extra), configuration management (js-yaml)

### API Design Requirements
[Source: architecture/api-design-and-integration.md#data-source-management]

**POST /api/v1/data-sources Endpoint:**
- Purpose: Create and configure new data source connections for PyAirbyte ingestion
- Integration: Extends existing CLI tool patterns, stores configuration in YAML following technical-preferences structure
- Authentication: Leverages existing BMad configuration patterns through technical-preferences.md
- Versioning: API versioning through URL path (`/api/v1/`) for backward compatibility

### Python Integration Standards
[Source: architecture/coding-standards-and-conventions.md#enhancement-specific-standards]

**Python Code Integration Standards:**
- All Python code executed through Node.js subprocess interfaces
- Python scripts follow PEP 8 standards with black formatting
- Virtual environment isolation with pinned dependency versions

**Error Handling Integration:**
- Python subprocess errors wrapped and translated to Node.js error objects
- Graceful degradation when data tools unavailable with clear user messaging
- All data processing errors use existing BMad error patterns with chalk styling

### File Structure Requirements
[Source: architecture/source-tree-integration.md#new-file-organization]

**New Files to Create:**
```plaintext
tools/
├── data-services/           # NEW: Data processing services
│   ├── data-ingestion-service.js
│   ├── pyairbyte-wrapper.js
│   └── connection-manager.js
├── lib/                     # Enhanced shared utilities
│   └── python-subprocess.js # NEW: Python process management
requirements.txt             # NEW: Python dependencies
.python-version             # NEW: Python version specification
```

### PyAirbyte Technical Details
[Source: architecture/tech-stack-alignment.md#new-technology-additions]

**PyAirbyte Version:** ^0.20.0  
**Purpose:** Flexible data ingestion  
**Rationale:** Major version update with improved caching and stream selection capabilities  
**Integration Method:** Python subprocess with JSON communication

**Key Capabilities to Implement:**
- Stream selection for selective data loading
- Cache management with connectivity to various sources
- Python-native integration with DataFrame libraries
- Connector architecture for diverse data sources

### Configuration Management
[Source: architecture/coding-standards-and-conventions.md#enhancement-specific-standards]

**Configuration Management Standards:**
- All data tool configurations stored in YAML following existing technical-preferences patterns
- Environment-specific settings isolated in separate configuration files
- Sensitive credentials encrypted using existing BMad security patterns

**Example Configuration Structure:**
```yaml
dataIngestion:
  pyairbyte:
    version: "^0.20.0"
    cacheLocation: ".cache/pyairbyte"
    maxCacheSize: "1GB"
  connectors:
    csv:
      enabled: true
      maxFileSize: "100MB"
    json:
      enabled: true
      maxNestingDepth: 10
    databases:
      postgresql:
        enabled: true
        poolSize: 5
      mysql:
        enabled: true
        poolSize: 5
```

### Memory and Performance Requirements
[Source: architecture/component-architecture.md#dataingestionservice]

**Performance Constraints:**
- PyAirbyte subprocess execution with JSON communication protocols
- Connection pooling for efficient database access
- Memory management for large file ingestion
- Non-blocking Node.js event loop during Python subprocess execution

### Integration Points with Existing System
[Source: architecture/coding-standards-and-conventions.md#critical-integration-rules]

**Existing API Compatibility:** All existing BMad-Method CLI commands must remain completely unchanged - new functionality is purely additive through expansion pack patterns

**CLI Integration Requirements:**
- Extend existing commander.js patterns for new data commands
- Follow existing tools/ directory organization patterns
- Maintain existing YAML configuration loading via yaml-utils.js
- Use existing fs-extra patterns for file operations

## Testing

### Testing Standards from Architecture
[Source: architecture/testing-strategy.md#new-testing-requirements]

**Framework:** Jest ^30.0.4 extended with Python subprocess testing utilities  
**Test Location:** `/tests/data-services/` following existing test organization patterns  
**Coverage Target:** 80% minimum coverage for all data processing components

**Specific Testing Requirements for This Story:**
- Python subprocess communication testing
- PyAirbyte connector validation testing  
- API endpoint integration testing
- Memory usage monitoring during ingestion
- Performance testing with various data sizes
- Regression testing for existing BMad CLI functionality

**Test Files to Create:**
- `/tests/data-services/data-ingestion-service.test.js` - Main service functionality tests
- `/tests/data-services/pyairbyte-wrapper.test.js` - Python subprocess wrapper tests  
- `/tests/data-services/connection-manager.test.js` - Database connection tests
- `/tests/integration/data-ingestion-api.test.js` - End-to-end API tests
- `/tests/performance/ingestion-memory-usage.test.js` - Memory and performance validation

**Integration Testing Scope:**
[Source: architecture/testing-strategy.md#integration-tests]
- Python-Node.js interoperability testing
- Cross-component data flow validation
- Existing System Verification: All existing BMad-Method functionality must continue working unchanged

**Critical Test Scenarios:**
- Large file ingestion without memory overflow
- Multiple concurrent data source connections
- PyAirbyte subprocess failure recovery
- Existing CLI tools continue functioning during data operations
- Cache management and invalidation workflows

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-08 | 1.0 | Initial story creation for PyAirbyte data ingestion | SM Bob |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*