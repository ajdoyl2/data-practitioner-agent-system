# Story 1.4: Transformation Workflows - dbt-core Integration

## Status
Draft

## Story
**As a** data engineer,
**I want** to define and execute transformation workflows using dbt-core,
**so that** I can maintain data quality and lineage.

## Acceptance Criteria
1. Integrate dbt-core with Python subprocess execution
2. Create guided ELT modeling templates (elt-modeling-guidance.md)
3. Implement dbt project initialization within expansion pack
4. Configure testing patterns (generic and custom tests)
5. Enable documentation generation from dbt models

## Integration Verification
- IV1: Existing validation scripts continue functioning
- IV2: dbt operations don't interfere with BMad build processes
- IV3: Test execution times remain reasonable (<5 minutes for standard suite)

## Tasks / Subtasks

- [ ] Task 1: Install and configure dbt-core environment (AC: 1)
  - [ ] Add dbt-core ^1.8.8 to requirements.txt with DuckDB adapter
  - [ ] Create Python subprocess wrapper for dbt command execution
  - [ ] Configure dbt project structure within expansion pack
  - [ ] Create dbt profiles.yml configuration for DuckDB connection
  - [ ] Implement dbt command execution with proper error handling
  - [ ] Test dbt installation and basic model compilation

- [ ] Task 2: Create TransformationEngine component (AC: 1, 3)
  - [ ] Create `tools/data-services/transformation-engine.js`
  - [ ] Implement dbt-core project initialization and model execution
  - [ ] Add dbt subprocess execution with JSON communication
  - [ ] Create dbt command wrapper functions (run, test, docs, compile)
  - [ ] Implement model dependency resolution and execution ordering
  - [ ] Add integration with existing task execution patterns

- [ ] Task 3: Implement dbt project initialization (AC: 3)
  - [ ] Create automated dbt project setup within bmad-data-practitioner
  - [ ] Generate dbt_project.yml with proper configuration
  - [ ] Create standard directory structure (models/, tests/, macros/, docs/)
  - [ ] Add DuckDB-specific dbt adapter configuration
  - [ ] Implement project validation and setup verification
  - [ ] Create project templates for different analysis types

- [ ] Task 4: Create guided ELT modeling templates (AC: 2)
  - [ ] Create `bmad-data-practitioner/tasks/elt-modeling-guidance.md`
  - [ ] Design layered architecture templates (Source → Staging → Intermediate → Marts)
  - [ ] Add template workflows for common transformation patterns
  - [ ] Create guided workflows following BMad elicitation patterns
  - [ ] Implement model generation helpers for standard patterns
  - [ ] Add model naming conventions and best practices guidance

- [ ] Task 5: Configure testing patterns (AC: 4)
  - [ ] Implement generic dbt tests (unique, not_null, accepted_values, relationships)
  - [ ] Create custom SQL-based test templates
  - [ ] Add data quality validation framework
  - [ ] Configure automated test generation based on model schemas
  - [ ] Implement test result parsing and reporting
  - [ ] Add integration with existing BMad validation systems

- [ ] Task 6: Enable documentation and lineage generation (AC: 5)
  - [ ] Configure dbt docs generation with custom styling
  - [ ] Implement lineage visualization integration
  - [ ] Create automated documentation updates
  - [ ] Add model description templates and standards
  - [ ] Integrate documentation with BMad documentation patterns
  - [ ] Configure docs hosting and access through BMad web-builder

- [ ] Task 7: Data transformation workflow integration
  - [ ] Create interface between AnalyticalEngine and TransformationEngine
  - [ ] Implement DuckDB source table registration for dbt models
  - [ ] Add incremental model support for large datasets
  - [ ] Create transformation job scheduling and monitoring
  - [ ] Implement transformation result validation and quality checks
  - [ ] Add transformation performance monitoring and optimization

- [ ] Task 8: Integration testing and validation (All IVs)
  - [ ] Verify existing validation scripts continue functioning (IV1)
  - [ ] Test dbt operations isolation from BMad build processes (IV2)
  - [ ] Monitor test execution times for performance compliance (IV3)
  - [ ] Run regression tests on existing BMad functionality
  - [ ] Performance testing for transformation workflows

## Dev Notes

### Previous Story Context
Stories 1.2 and 1.3 implemented data ingestion (PyAirbyte) and analytical processing (DuckDB). This story builds the transformation layer between raw ingested data and analytics-ready datasets using dbt-core for maintaining data quality and lineage.

### TransformationEngine Architecture
[Source: architecture/component-architecture.md#transformationengine]

**Responsibility:** Executes dbt-core transformation workflows, manages data lineage tracking, and maintains data quality through comprehensive testing

**Key Interfaces Required:**
- dbt-core project initialization and model execution
- Guided ELT modeling templates following BMad elicitation patterns
- Data quality testing framework with automated test generation
- Lineage visualization and documentation generation

**Technology Stack Dependencies:**
- dbt-core ^1.8.8 (data transformation framework)
- Python subprocess execution for dbt command processing
- YAML-based configuration following BMad patterns
- Existing Components: Task execution framework, template engine, validation systems
- New Components: AnalyticalEngine (data sources - Story 1.3), WorkflowOrchestrator (pipeline coordination - Story 1.5)

### dbt-core Technical Specifications
[Source: architecture/tech-stack-alignment.md#new-technology-additions]

**dbt-core Version:** ^1.8.8  
**Purpose:** Data transformation workflows  
**Rationale:** Latest stable with improved Jinja rendering and enhanced testing framework  
**Integration Method:** Python subprocess execution

**Key Capabilities to Implement:**
- Layered architecture approach (Source → Staging → Intermediate → Marts)
- Generic tests (unique, not_null, accepted_values, relationships) and custom SQL-based assertions
- CI/CD integration patterns with selective execution for modified models
- Comprehensive testing patterns and documentation generation

### DataTransformation Model Integration
[Source: architecture/data-models-and-schema-changes.md#datatransformation]

**DataTransformation Model Structure:**
- transformation_id: String - Unique identifier for dbt model tracking
- model_name: String - dbt model name following naming conventions
- source_references: Array[String] - References to DataSource entities from Stories 1.2/1.3
- transformation_logic: String - SQL transformation logic (stored in dbt models)
- test_results: Object - dbt test execution results and quality metrics
- lineage_metadata: Object - Upstream and downstream dependencies
- execution_metadata: Object - Performance and resource usage tracking

**Integration Relationships:**
- With Existing: Follows existing task workflow patterns for execution tracking
- With New: Child of AnalysisProject, feeds into InsightDocument generation (Story 1.7)

### File Structure Requirements
[Source: architecture/source-tree-integration.md#new-file-organization]

**New Files to Create:**
```plaintext
expansion-packs/bmad-data-practitioner/
├── dbt-project/                    # NEW: dbt project structure
│   ├── dbt_project.yml
│   ├── profiles.yml
│   ├── models/
│   │   ├── staging/
│   │   ├── intermediate/
│   │   └── marts/
│   ├── tests/
│   ├── macros/
│   └── docs/
tools/data-services/
├── transformation-engine.js        # NEW: dbt operations and workflow management
├── dbt-wrapper.js                 # NEW: dbt subprocess execution wrapper
└── model-generator.js             # NEW: Model template generation helpers
```

### dbt Project Configuration
**DuckDB Integration Configuration:**
```yaml
# dbt_project.yml
name: 'bmad_data_practitioner'
version: '1.0.0'
config-version: 2

model-paths: ["models"]
analysis-paths: ["analysis"]
test-paths: ["tests"]
seed-paths: ["data"]
macro-paths: ["macros"]

target-path: "target"
clean-targets: ["target", "dbt_packages"]

models:
  bmad_data_practitioner:
    staging:
      +materialized: view
    intermediate:
      +materialized: view
    marts:
      +materialized: table

# profiles.yml
bmad_data_practitioner:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: '../../../.duckdb/analytics.db'
      threads: 4
```

### Layered Architecture Implementation
**ELT Modeling Structure:**
- **Source Layer**: Raw data from PyAirbyte ingestion, registered as dbt sources
- **Staging Layer**: One-to-one with source tables, basic cleansing and type casting
- **Intermediate Layer**: Business logic transformations, joins, and aggregations
- **Marts Layer**: Final analytics-ready datasets for consumption

**Model Naming Conventions:**
- Staging: `stg_[source_name]__[table_name]`
- Intermediate: `int_[business_concept]__[description]`
- Marts: `[business_area]__[entity_type]` (e.g., `sales__customer_metrics`)

### Testing Framework Implementation
**Generic Test Configuration:**
- unique: Ensure column values are unique
- not_null: Ensure column values are not null
- accepted_values: Validate column values against allowed list
- relationships: Validate foreign key relationships

**Custom Test Patterns:**
- Data quality thresholds (completeness, accuracy, consistency)
- Business rule validation (e.g., positive revenue amounts)
- Statistical outlier detection
- Cross-table consistency checks

### Python Integration Standards
[Source: architecture/coding-standards-and-conventions.md#enhancement-specific-standards]

**Python Code Integration Standards:**
- All dbt commands executed through Node.js subprocess interfaces
- dbt project follows standard conventions with DuckDB adapter
- Virtual environment isolation with pinned dependency versions
- Error handling with proper subprocess management

**dbt Command Execution Patterns:**
```javascript
// Example dbt subprocess execution pattern
const dbtRun = async (models = null, fullRefresh = false) => {
  const args = ['run'];
  if (models) args.push('--models', models);
  if (fullRefresh) args.push('--full-refresh');
  
  return await executeDbtCommand(args);
};
```

### Integration with BMad Template System
[Source: architecture/component-architecture.md#transformationengine]

**Guided ELT Modeling Templates:**
- Integration with BMad template system for guided ELT workflows
- Elicitation patterns for model generation and configuration
- Template-driven model creation with validation
- Interactive workflows for transformation design

### Performance and Quality Requirements
**Data Quality Framework:**
- Automated test generation based on model schemas and business rules
- Comprehensive testing patterns with 80% minimum coverage
- Test result parsing and integration with BMad validation systems
- Performance monitoring for transformation execution times

**Transformation Optimization:**
- Incremental model support for large datasets
- Query optimization recommendations based on DuckDB capabilities
- Resource usage monitoring and optimization suggestions
- Selective model execution for development efficiency

### Error Handling and Process Management
[Source: architecture/coding-standards-and-conventions.md#enhancement-specific-standards]

**dbt Operations Error Handling:**
- Python subprocess errors wrapped and translated to Node.js error objects
- Comprehensive error handling for dbt compilation, testing, and execution failures
- Graceful degradation when dbt tools unavailable with clear user messaging
- Process timeout handling for long-running transformations

### Integration Points with Existing System
[Source: architecture/coding-standards-and-conventions.md#critical-integration-rules]

**Existing API Compatibility:** All existing BMad-Method validation scripts and build processes must remain completely unchanged - dbt operations are isolated within the data processing pipeline

**BMad Integration Requirements:**
- Follow existing task execution patterns for transformation workflows
- Integrate with template engine for guided modeling workflows
- Use existing validation systems for transformation quality gates
- Maintain existing progress tracking patterns (ora, chalk)

## Testing

### Testing Standards from Architecture
[Source: architecture/testing-strategy.md#new-testing-requirements]

**Framework:** Jest ^30.0.4 extended with dbt testing utilities and subprocess validation  
**Test Location:** `/tests/data-services/` following existing test organization patterns  
**Coverage Target:** 80% minimum coverage for all data processing components

**Specific Testing Requirements for This Story:**
- dbt subprocess execution and command wrapper testing
- Model compilation and execution validation
- Data quality testing framework verification
- Transformation lineage and documentation generation testing
- Performance testing for transformation execution times
- Integration testing with AnalyticalEngine (DuckDB) from Story 1.3

**Test Files to Create:**
- `/tests/data-services/transformation-engine.test.js` - Core dbt operations testing
- `/tests/data-services/dbt-wrapper.test.js` - dbt subprocess wrapper functionality
- `/tests/data-services/model-generator.test.js` - Model template generation testing
- `/tests/data-services/data-quality-testing.test.js` - Testing framework validation
- `/tests/integration/duckdb-dbt-pipeline.test.js` - End-to-end transformation pipeline
- `/tests/performance/transformation-execution-time.test.js` - Performance compliance validation

**Integration Testing Scope:**
[Source: architecture/testing-strategy.md#integration-tests]
- Complete data transformation pipeline testing (DuckDB → dbt → transformed datasets)
- Cross-component data flow validation with AnalyticalEngine
- Existing System Verification: All existing BMad-Method functionality must continue working unchanged

**Critical Test Scenarios:**
- dbt project initialization and configuration validation
- Model compilation and execution across all layers (staging, intermediate, marts)
- Data quality testing with various test patterns and custom validations
- Documentation generation and lineage visualization
- Incremental model execution with large datasets
- Error handling for dbt compilation and execution failures
- Performance compliance for test execution times (<5 minutes)
- Integration with existing BMad validation scripts
- dbt operations isolation from BMad build processes

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-08 | 1.0 | Initial story creation for dbt-core transformation workflows | SM Bob |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*