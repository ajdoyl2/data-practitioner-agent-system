# Cross-Tool Testing Patterns

Advanced testing patterns for multi-tool data stack integration with comprehensive validation strategies for the complete PyAirbyte â†’ DuckDB â†’ SQLmesh/dbt â†’ Dagster â†’ Evidence.dev pipeline.

## ðŸŽ¯ Cross-Tool Testing Strategy

### Testing Philosophy

**End-to-End Data Flow Validation:**
- **Data Lineage Testing**: Validate data integrity across tool boundaries
- **Performance Chain Testing**: Ensure performance remains optimal across the full stack
- **Error Propagation Testing**: Verify proper error handling and recovery across tools
- **Configuration Consistency**: Validate configuration alignment across all components

**Tool Chain Integration Points:**
1. **PyAirbyte â†’ DuckDB**: Data ingestion and type mapping
2. **DuckDB â†’ SQLmesh/dbt**: Transformation engine coordination
3. **SQLmesh/dbt â†’ Dagster**: Orchestration and scheduling
4. **Dagster â†’ Evidence.dev**: Report generation and deployment
5. **Cross-Tool Monitoring**: Health checks and performance monitoring

---

## ðŸ”§ Advanced Integration Test Patterns

### Dagster â†’ Evidence.dev Integration Tests

#### Orchestration to Reporting Pipeline

```bash
# Create Dagster to Evidence.dev integration tests
cat > tests/integration/test_dagster_evidence_integration.py << 'EOF'
#!/usr/bin/env python3
"""
Dagster to Evidence.dev Integration Tests
Tests orchestration of data pipelines with automated report generation
"""

import unittest
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'utilities'))

from base_test_framework import DataPractitionerIntegrationTest
from test_environment_setup import TestEnvironment
import subprocess
import json
import time
from pathlib import Path
import yaml

class DagsterEvidenceIntegrationTest(DataPractitionerIntegrationTest):
    """Integration tests for Dagster â†’ Evidence.dev pipeline"""
    
    @classmethod
    def setUpClass(cls):
        """Set up test environment"""
        super().setUpClass()
        cls.test_env = TestEnvironment('integration').__enter__()
        
    @classmethod
    def tearDownClass(cls):
        """Clean up test environment"""
        TestEnvironment('integration').__exit__(None, None, None)
        super().tearDownClass()
    
    def setUp(self):
        """Set up individual test"""
        super().setUp()
        self.dagster_home = self.test_env['paths']['temp_root'] / 'dagster_home'
        self.evidence_project = self.test_env['paths']['evidence_project']
        self.db_path = self.test_env['databases']['integration_test_db']
        self._setup_dagster_environment()
        self._setup_evidence_project()
        
    def _setup_dagster_environment(self):
        """Set up Dagster environment for testing"""
        self.dagster_home.mkdir(exist_ok=True)
        os.environ['DAGSTER_HOME'] = str(self.dagster_home)
        
        # Create Dagster workspace
        workspace_content = f"""
load_from:
  - python_file: {self._create_test_dagster_pipeline()}
"""
        workspace_file = self.dagster_home / 'workspace.yaml'
        with open(workspace_file, 'w') as f:
            f.write(workspace_content)
    
    def _create_test_dagster_pipeline(self):
        """Create test Dagster pipeline"""
        pipeline_dir = self.test_env['paths']['temp_root'] / 'dagster_pipelines'
        pipeline_dir.mkdir(exist_ok=True)
        
        pipeline_content = f'''
import duckdb
from dagster import asset, Definitions, AssetExecutionContext
from pathlib import Path

@asset
def customer_data():
    """Load customer data into DuckDB"""
    conn = duckdb.connect("{self.db_path}")
    
    # Load sample data
    customers_path = "{self.test_env['sample_data']['customers']}"
    conn.execute(f"""
        CREATE OR REPLACE TABLE customers AS 
        SELECT * FROM read_csv_auto('{{customers_path}}')
    """)
    
    count = conn.execute("SELECT COUNT(*) FROM customers").fetchone()[0]
    conn.close()
    
    return {{"table": "customers", "row_count": count}}

@asset
def order_data():
    """Load order data into DuckDB"""
    conn = duckdb.connect("{self.db_path}")
    
    # Load sample data
    orders_path = "{self.test_env['sample_data']['orders']}"
    conn.execute(f"""
        CREATE OR REPLACE TABLE orders AS 
        SELECT * FROM read_csv_auto('{{orders_path}}')
    """)
    
    count = conn.execute("SELECT COUNT(*) FROM orders").fetchone()[0]
    conn.close()
    
    return {{"table": "orders", "row_count": count}}

@asset(deps=[customer_data, order_data])
def customer_analytics():
    """Create customer analytics summary"""
    conn = duckdb.connect("{self.db_path}")
    
    conn.execute("""
        CREATE OR REPLACE TABLE customer_analytics AS
        SELECT 
            c.customer_id,
            c.name,
            c.email,
            COUNT(o.order_id) as total_orders,
            COALESCE(SUM(o.amount), 0) as total_spent,
            COALESCE(AVG(o.amount), 0) as avg_order_value,
            MAX(o.order_date) as last_order_date
        FROM customers c
        LEFT JOIN orders o ON c.customer_id = o.customer_id
        GROUP BY c.customer_id, c.name, c.email
    """)
    
    count = conn.execute("SELECT COUNT(*) FROM customer_analytics").fetchone()[0]
    conn.close()
    
    return {{"table": "customer_analytics", "row_count": count}}

@asset(deps=[customer_analytics])
def evidence_report():
    """Generate Evidence.dev report"""
    import subprocess
    import time
    
    evidence_project_path = "{self.evidence_project}"
    
    # Trigger Evidence.dev build (simulated)
    # In real implementation: subprocess.run(["evidence", "build"], cwd=evidence_project_path)
    
    # For testing, we'll create a mock build output
    build_dir = Path(evidence_project_path) / "build"
    build_dir.mkdir(exist_ok=True)
    
    # Create mock report file
    report_file = build_dir / "customer-dashboard.html"
    with open(report_file, 'w') as f:
        f.write("""
        <html>
        <head><title>Customer Analytics Dashboard</title></head>
        <body>
            <h1>Customer Analytics Dashboard</h1>
            <p>Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p>Data source: customer_analytics table</p>
        </body>
        </html>
        """)
    
    return {{
        "report_path": str(report_file),
        "build_time": time.time(),
        "status": "success"
    }}

defs = Definitions(
    assets=[customer_data, order_data, customer_analytics, evidence_report]
)
'''
        
        pipeline_file = pipeline_dir / 'test_pipeline.py'
        with open(pipeline_file, 'w') as f:
            f.write(pipeline_content)
        
        return str(pipeline_file)
    
    def _setup_evidence_project(self):
        """Set up Evidence.dev project structure"""
        # Create Evidence.dev project structure
        pages_dir = self.evidence_project / 'pages'
        pages_dir.mkdir(parents=True, exist_ok=True)
        
        sources_dir = self.evidence_project / 'sources'
        sources_dir.mkdir(exist_ok=True)
        
        # Create Evidence.dev configuration
        evidence_config = {
            'database': {
                'type': 'duckdb',
                'filename': str(self.db_path)
            },
            'build': {
                'output_dir': 'build'
            }
        }
        
        config_file = self.evidence_project / 'evidence.settings.json'
        with open(config_file, 'w') as f:
            json.dump(evidence_config, f, indent=2)
        
        # Create sample dashboard page
        dashboard_content = '''
# Customer Analytics Dashboard

## Overview
This dashboard provides insights into customer behavior and purchasing patterns.

```sql customers_summary
SELECT 
    COUNT(*) as total_customers,
    ROUND(AVG(total_spent), 2) as avg_customer_value,
    ROUND(SUM(total_spent), 2) as total_revenue
FROM customer_analytics
```

### Key Metrics

<BigValue 
    data={customers_summary} 
    value=total_customers 
    title="Total Customers"
/>

<BigValue 
    data={customers_summary} 
    value=avg_customer_value 
    title="Avg Customer Value"
    fmt="$0,000.00"
/>

<BigValue 
    data={customers_summary} 
    value=total_revenue 
    title="Total Revenue"
    fmt="$0,000.00"
/>

## Customer Segmentation

```sql customer_segments
SELECT 
    CASE 
        WHEN total_orders >= 5 THEN 'High Value'
        WHEN total_orders >= 2 THEN 'Medium Value'
        ELSE 'Low Value'
    END as segment,
    COUNT(*) as customer_count,
    ROUND(AVG(total_spent), 2) as avg_spent
FROM customer_analytics
GROUP BY segment
ORDER BY avg_spent DESC
```

<BarChart 
    data={customer_segments} 
    x=segment 
    y=customer_count 
    title="Customer Distribution by Segment"
/>
'''
        
        dashboard_file = pages_dir / 'customer-dashboard.md'
        with open(dashboard_file, 'w') as f:
            f.write(dashboard_content)
    
    def test_full_pipeline_execution(self):
        """Test complete pipeline from Dagster orchestration to Evidence.dev report"""
        with self.measure_execution_time("full_pipeline_execution"):
            # Simulate Dagster pipeline execution
            # In real implementation: dagster-daemon would orchestrate this
            
            # Step 1: Execute data loading assets
            self._execute_dagster_asset("customer_data")
            self._execute_dagster_asset("order_data")
            
            # Step 2: Execute transformation asset
            self._execute_dagster_asset("customer_analytics")
            
            # Step 3: Execute report generation asset
            report_result = self._execute_dagster_asset("evidence_report")
        
        # Verify pipeline execution
        self.assertIsNotNone(report_result, "Evidence report generation failed")
        self.assertIn("report_path", report_result, "Report path not returned")
        
        # Verify report file was created
        report_path = Path(report_result["report_path"])
        self.assert_file_exists(str(report_path), "Evidence.dev report file not created")
        
        # Performance assertion
        self.assert_performance_within_limits(
            self.performance_metrics["full_pipeline_execution"],
            60.0,
            "Full pipeline execution"
        )
    
    def _execute_dagster_asset(self, asset_name: str):
        """Simulate Dagster asset execution"""
        # In a real implementation, this would use Dagster's execution API
        # For testing purposes, we'll simulate the asset execution
        
        if asset_name == "customer_data":
            import duckdb
            conn = duckdb.connect(self.db_path)
            customers_path = self.test_env['sample_data']['customers']
            conn.execute(f"""
                CREATE OR REPLACE TABLE customers AS 
                SELECT * FROM read_csv_auto('{customers_path}')
            """)
            count = conn.execute("SELECT COUNT(*) FROM customers").fetchone()[0]
            conn.close()
            return {"table": "customers", "row_count": count}
        
        elif asset_name == "order_data":
            import duckdb
            conn = duckdb.connect(self.db_path)
            orders_path = self.test_env['sample_data']['orders']
            conn.execute(f"""
                CREATE OR REPLACE TABLE orders AS 
                SELECT * FROM read_csv_auto('{orders_path}')
            """)
            count = conn.execute("SELECT COUNT(*) FROM orders").fetchone()[0]
            conn.close()
            return {"table": "orders", "row_count": count}
        
        elif asset_name == "customer_analytics":
            import duckdb
            conn = duckdb.connect(self.db_path)
            conn.execute("""
                CREATE OR REPLACE TABLE customer_analytics AS
                SELECT 
                    c.customer_id,
                    c.name,
                    c.email,
                    COUNT(o.order_id) as total_orders,
                    COALESCE(SUM(o.amount), 0) as total_spent,
                    COALESCE(AVG(o.amount), 0) as avg_order_value,
                    MAX(o.order_date) as last_order_date
                FROM customers c
                LEFT JOIN orders o ON c.customer_id = o.customer_id
                GROUP BY c.customer_id, c.name, c.email
            """)
            count = conn.execute("SELECT COUNT(*) FROM customer_analytics").fetchone()[0]
            conn.close()
            return {"table": "customer_analytics", "row_count": count}
        
        elif asset_name == "evidence_report":
            # Generate Evidence.dev report
            build_dir = self.evidence_project / "build"
            build_dir.mkdir(exist_ok=True)
            
            report_file = build_dir / "customer-dashboard.html"
            with open(report_file, 'w') as f:
                f.write(f"""
                <html>
                <head><title>Customer Analytics Dashboard</title></head>
                <body>
                    <h1>Customer Analytics Dashboard</h1>
                    <p>Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
                    <p>Data source: customer_analytics table</p>
                </body>
                </html>
                """)
            
            return {
                "report_path": str(report_file),
                "build_time": time.time(),
                "status": "success"
            }
        
        return None
    
    def test_pipeline_dependency_resolution(self):
        """Test that Dagster properly resolves asset dependencies"""
        # Test dependency chain: customer_data & order_data â†’ customer_analytics â†’ evidence_report
        
        dependency_chain = []
        
        # Execute in dependency order
        with self.measure_execution_time("dependency_resolution"):
            # First level: independent assets
            result1 = self._execute_dagster_asset("customer_data")
            dependency_chain.append(("customer_data", result1))
            
            result2 = self._execute_dagster_asset("order_data")
            dependency_chain.append(("order_data", result2))
            
            # Second level: depends on first level
            result3 = self._execute_dagster_asset("customer_analytics")
            dependency_chain.append(("customer_analytics", result3))
            
            # Third level: depends on second level
            result4 = self._execute_dagster_asset("evidence_report")
            dependency_chain.append(("evidence_report", result4))
        
        # Verify all assets executed successfully
        for asset_name, result in dependency_chain:
            self.assertIsNotNone(result, f"Asset {asset_name} execution failed")
        
        # Verify data flow continuity
        self.assertGreater(result1["row_count"], 0, "Customer data not loaded")
        self.assertGreater(result2["row_count"], 0, "Order data not loaded")
        self.assertGreater(result3["row_count"], 0, "Customer analytics not created")
        self.assertEqual(result4["status"], "success", "Evidence report generation failed")
    
    def test_error_handling_and_recovery(self):
        """Test error handling across tool boundaries"""
        # Test 1: Database connection failure
        invalid_db_path = "/invalid/path/test.duckdb"
        
        with self.assertRaises(Exception):
            conn = __import__('duckdb').connect(invalid_db_path)
            conn.execute("SELECT 1")
        
        # Test 2: Missing dependency handling
        # Try to execute dependent asset without prerequisites
        with self.assertRaises(Exception):
            # This should fail because customer_analytics depends on customer_data and order_data
            import duckdb
            conn = duckdb.connect(self.db_path)
            conn.execute("SELECT COUNT(*) FROM customer_analytics")  # Table doesn't exist yet
            conn.close()
        
        # Test 3: Evidence.dev build failure simulation
        invalid_evidence_project = self.create_temp_directory() / "invalid_evidence"
        
        # Missing configuration should cause build failure
        self.assertFalse(
            (invalid_evidence_project / "evidence.settings.json").exists(),
            "Invalid Evidence project should not have config"
        )
    
    def test_incremental_pipeline_updates(self):
        """Test incremental updates through the pipeline"""
        # Initial pipeline run
        self._execute_dagster_asset("customer_data")
        self._execute_dagster_asset("order_data")
        self._execute_dagster_asset("customer_analytics")
        
        # Get initial counts
        import duckdb
        conn = duckdb.connect(self.db_path)
        initial_customer_count = conn.execute("SELECT COUNT(*) FROM customers").fetchone()[0]
        initial_analytics_count = conn.execute("SELECT COUNT(*) FROM customer_analytics").fetchone()[0]
        
        # Add new data
        conn.execute("""
            INSERT INTO customers VALUES 
            (99999, 'New Customer', 'new@example.com', '2024-01-01', 0, 0.00)
        """)
        
        conn.execute("""
            INSERT INTO orders VALUES 
            (99999, 99999, '2024-01-01', 'electronics', 299.99, 1)
        """)
        
        conn.close()
        
        # Re-run pipeline with incremental data
        with self.measure_execution_time("incremental_update"):
            self._execute_dagster_asset("customer_analytics")  # Re-run analytics
            report_result = self._execute_dagster_asset("evidence_report")  # Re-generate report
        
        # Verify incremental updates
        conn = duckdb.connect(self.db_path)
        updated_analytics_count = conn.execute("SELECT COUNT(*) FROM customer_analytics").fetchone()[0]
        
        # Check that new customer appears in analytics
        new_customer_analytics = conn.execute("""
            SELECT total_orders, total_spent 
            FROM customer_analytics 
            WHERE customer_id = 99999
        """).fetchone()
        
        conn.close()
        
        # Assertions
        self.assertEqual(updated_analytics_count, initial_analytics_count + 1, 
                        "Incremental customer not added to analytics")
        self.assertIsNotNone(new_customer_analytics, "New customer not in analytics")
        self.assertEqual(new_customer_analytics[0], 1, "New customer order count incorrect")
        self.assertEqual(new_customer_analytics[1], 299.99, "New customer total spent incorrect")
        
        # Performance assertion
        self.assert_performance_within_limits(
            self.performance_metrics["incremental_update"],
            30.0,
            "Incremental pipeline update"
        )
    
    def test_report_content_validation(self):
        """Test that Evidence.dev reports contain expected content"""
        # Execute full pipeline
        self._execute_dagster_asset("customer_data")
        self._execute_dagster_asset("order_data")
        self._execute_dagster_asset("customer_analytics")
        report_result = self._execute_dagster_asset("evidence_report")
        
        # Read generated report
        report_path = Path(report_result["report_path"])
        with open(report_path, 'r') as f:
            report_content = f.read()
        
        # Validate report content
        self.assertIn("Customer Analytics Dashboard", report_content, 
                     "Report missing main title")
        self.assertIn("Generated at:", report_content, 
                     "Report missing generation timestamp")
        self.assertIn("customer_analytics", report_content, 
                     "Report missing data source reference")
        
        # Validate HTML structure
        self.assertIn("<html>", report_content, "Report missing HTML structure")
        self.assertIn("<title>", report_content, "Report missing title tag")
        self.assertIn("</body>", report_content, "Report missing closing body tag")
    
    def test_performance_monitoring_across_tools(self):
        """Test performance monitoring across the entire tool chain"""
        performance_metrics = {}
        
        # Measure each stage of the pipeline
        with self.measure_execution_time("data_loading_stage"):
            self._execute_dagster_asset("customer_data")
            self._execute_dagster_asset("order_data")
        performance_metrics["data_loading"] = self.performance_metrics["data_loading_stage"]
        
        with self.measure_execution_time("transformation_stage"):
            self._execute_dagster_asset("customer_analytics")
        performance_metrics["transformation"] = self.performance_metrics["transformation_stage"]
        
        with self.measure_execution_time("reporting_stage"):
            self._execute_dagster_asset("evidence_report")
        performance_metrics["reporting"] = self.performance_metrics["reporting_stage"]
        
        # Calculate total pipeline time
        total_time = sum(performance_metrics.values())
        
        # Performance assertions
        self.assert_performance_within_limits(total_time, 90.0, "Complete pipeline")
        
        # Individual stage assertions
        self.assert_performance_within_limits(
            performance_metrics["data_loading"], 20.0, "Data loading stage"
        )
        self.assert_performance_within_limits(
            performance_metrics["transformation"], 30.0, "Transformation stage"
        )
        self.assert_performance_within_limits(
            performance_metrics["reporting"], 15.0, "Reporting stage"
        )
        
        # Log performance breakdown
        print(f"\nðŸ“Š Pipeline Performance Breakdown:")
        print(f"  Data Loading: {performance_metrics['data_loading']:.2f}s")
        print(f"  Transformation: {performance_metrics['transformation']:.2f}s")
        print(f"  Reporting: {performance_metrics['reporting']:.2f}s")
        print(f"  Total Pipeline: {total_time:.2f}s")

if __name__ == "__main__":
    unittest.main(verbosity=2)
EOF

chmod +x tests/integration/test_dagster_evidence_integration.py

echo "âœ… Dagster â†’ Evidence.dev integration tests created"
```

### Complete End-to-End Pipeline Tests

#### Full Stack Integration Validation

```bash
# Create complete end-to-end pipeline tests
cat > tests/integration/test_complete_pipeline_integration.py << 'EOF'
#!/usr/bin/env python3
"""
Complete Pipeline Integration Tests
Tests the entire PyAirbyte â†’ DuckDB â†’ SQLmesh/dbt â†’ Dagster â†’ Evidence.dev pipeline
"""

import unittest
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'utilities'))

from base_test_framework import DataPractitionerIntegrationTest
from test_environment_setup import TestEnvironment
import json
import time
import csv
from pathlib import Path
import duckdb

class CompletePipelineIntegrationTest(DataPractitionerIntegrationTest):
    """End-to-end integration tests for the complete data pipeline"""
    
    @classmethod
    def setUpClass(cls):
        """Set up test environment"""
        super().setUpClass()
        cls.test_env = TestEnvironment('system').__enter__()
        
    @classmethod
    def tearDownClass(cls):
        """Clean up test environment"""
        TestEnvironment('system').__exit__(None, None, None)
        super().tearDownClass()
    
    def setUp(self):
        """Set up individual test"""
        super().setUp()
        self.pipeline_state = {
            'stages': [],
            'data_lineage': {},
            'performance_metrics': {},
            'quality_checks': {}
        }
        
    def test_complete_pipeline_flow(self):
        """Test complete data flow from ingestion to reporting"""
        print("\nðŸš€ Starting complete pipeline integration test...")
        
        with self.measure_execution_time("complete_pipeline"):
            # Stage 1: Data Ingestion (PyAirbyte simulation)
            ingestion_result = self._execute_data_ingestion()
            self._track_pipeline_stage("data_ingestion", {}, ingestion_result)
            
            # Stage 2: Data Storage (DuckDB)
            storage_result = self._execute_data_storage(ingestion_result)
            self._track_pipeline_stage("data_storage", ingestion_result, storage_result)
            
            # Stage 3: Data Transformation (SQLmesh/dbt)
            transformation_result = self._execute_data_transformation(storage_result)
            self._track_pipeline_stage("data_transformation", storage_result, transformation_result)
            
            # Stage 4: Orchestration (Dagster)
            orchestration_result = self._execute_orchestration(transformation_result)
            self._track_pipeline_stage("orchestration", transformation_result, orchestration_result)
            
            # Stage 5: Reporting (Evidence.dev)
            reporting_result = self._execute_reporting(orchestration_result)
            self._track_pipeline_stage("reporting", orchestration_result, reporting_result)
        
        # Validate complete pipeline
        self._validate_pipeline_completion()
        
        # Performance validation
        self.assert_performance_within_limits(
            self.performance_metrics["complete_pipeline"],
            300.0,  # 5 minutes max for complete pipeline
            "Complete pipeline execution"
        )
        
        print("âœ… Complete pipeline integration test passed!")
    
    def _execute_data_ingestion(self):
        """Simulate PyAirbyte data ingestion"""
        print("ðŸ“¥ Executing data ingestion stage...")
        
        with self.measure_execution_time("data_ingestion_stage"):
            # Create enhanced sample data to simulate PyAirbyte extraction
            temp_dir = self.create_temp_directory("ingestion_")
            
            # Generate customers data with more realistic structure
            customers_file = temp_dir / "customers_extracted.csv"
            self._generate_realistic_customers(customers_file, 1000)
            
            # Generate orders data
            orders_file = temp_dir / "orders_extracted.csv"
            self._generate_realistic_orders(orders_file, 3000)
            
            # Generate products data
            products_file = temp_dir / "products_extracted.csv"
            self._generate_realistic_products(products_file, 100)
            
            # Simulate PyAirbyte metadata
            extraction_metadata = {
                "extraction_time": time.time(),
                "source_system": "test_erp",
                "connector_version": "0.20.0",
                "extracted_files": {
                    "customers": str(customers_file),
                    "orders": str(orders_file),
                    "products": str(products_file)
                },
                "record_counts": {
                    "customers": 1000,
                    "orders": 3000,
                    "products": 100
                }
            }
        
        return extraction_metadata
    
    def _generate_realistic_customers(self, file_path: Path, num_records: int):
        """Generate realistic customer data"""
        import random
        from datetime import datetime, timedelta
        
        countries = ['USA', 'Canada', 'UK', 'Germany', 'France', 'Australia', 'Japan']
        tiers = ['basic', 'premium', 'enterprise']
        
        with open(file_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                'customer_id', 'name', 'email', 'signup_date', 'country', 
                'subscription_tier', 'total_orders', 'total_spent', 'last_login'
            ])
            
            for i in range(1, num_records + 1):
                signup_date = datetime.now() - timedelta(days=random.randint(1, 1095))  # 3 years
                last_login = signup_date + timedelta(days=random.randint(0, 30))
                
                writer.writerow([
                    i,
                    f"Customer {i:04d}",
                    f"customer{i:04d}@example.com",
                    signup_date.strftime('%Y-%m-%d'),
                    random.choice(countries),
                    random.choice(tiers),
                    random.randint(0, 50),
                    round(random.uniform(0, 5000), 2),
                    last_login.strftime('%Y-%m-%d %H:%M:%S')
                ])
    
    def _generate_realistic_orders(self, file_path: Path, num_records: int):
        """Generate realistic order data"""
        import random
        from datetime import datetime, timedelta
        
        categories = ['electronics', 'clothing', 'books', 'home', 'sports', 'beauty', 'automotive']
        statuses = ['completed', 'pending', 'cancelled', 'refunded']
        
        with open(file_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                'order_id', 'customer_id', 'order_date', 'product_category', 
                'amount', 'quantity', 'status', 'shipping_country'
            ])
            
            for i in range(1, num_records + 1):
                order_date = datetime.now() - timedelta(days=random.randint(1, 365))
                
                writer.writerow([
                    i,
                    random.randint(1, 1000),  # customer_id from customers table
                    order_date.strftime('%Y-%m-%d'),
                    random.choice(categories),
                    round(random.uniform(10, 1000), 2),
                    random.randint(1, 10),
                    random.choice(statuses),
                    random.choice(['USA', 'Canada', 'UK', 'Germany', 'France'])
                ])
    
    def _generate_realistic_products(self, file_path: Path, num_records: int):
        """Generate realistic product data"""
        import random
        
        categories = ['electronics', 'clothing', 'books', 'home', 'sports', 'beauty', 'automotive']
        
        with open(file_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                'product_id', 'name', 'category', 'price', 'cost', 
                'stock_quantity', 'supplier', 'is_active'
            ])
            
            for i in range(1, num_records + 1):
                category = random.choice(categories)
                price = round(random.uniform(10, 500), 2)
                cost = round(price * random.uniform(0.3, 0.7), 2)
                
                writer.writerow([
                    i,
                    f"{category.title()} Product {i:03d}",
                    category,
                    price,
                    cost,
                    random.randint(0, 500),
                    f"Supplier {random.randint(1, 20):02d}",
                    random.choice([True, False])
                ])
    
    def _execute_data_storage(self, ingestion_result):
        """Execute DuckDB data storage"""
        print("ðŸ—„ï¸ Executing data storage stage...")
        
        with self.measure_execution_time("data_storage_stage"):
            db_path = self.test_env['databases']['system_test_db']
            conn = duckdb.connect(db_path)
            
            storage_result = {
                "database_path": db_path,
                "tables_created": [],
                "row_counts": {},
                "storage_time": time.time()
            }
            
            # Load each extracted file into DuckDB
            for table_name, file_path in ingestion_result["extracted_files"].items():
                conn.execute(f"""
                    CREATE OR REPLACE TABLE {table_name} AS 
                    SELECT * FROM read_csv_auto('{file_path}')
                """)
                
                # Get row count
                row_count = conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
                
                storage_result["tables_created"].append(table_name)
                storage_result["row_counts"][table_name] = row_count
                
                print(f"  âœ… Loaded {row_count} rows into {table_name} table")
            
            # Create indexes for performance
            conn.execute("CREATE INDEX idx_customers_id ON customers(customer_id)")
            conn.execute("CREATE INDEX idx_orders_customer ON orders(customer_id)")
            conn.execute("CREATE INDEX idx_orders_date ON orders(order_date)")
            
            conn.close()
        
        return storage_result
    
    def _execute_data_transformation(self, storage_result):
        """Execute SQLmesh/dbt data transformation"""
        print("ðŸ”„ Executing data transformation stage...")
        
        with self.measure_execution_time("transformation_stage"):
            db_path = storage_result["database_path"]
            conn = duckdb.connect(db_path)
            
            transformation_result = {
                "models_created": [],
                "transformation_time": time.time(),
                "quality_metrics": {}
            }
            
            # Create customer analytics model
            conn.execute("""
                CREATE OR REPLACE TABLE customer_analytics AS
                SELECT 
                    c.customer_id,
                    c.name,
                    c.email,
                    c.country,
                    c.subscription_tier,
                    c.signup_date,
                    COUNT(o.order_id) as order_count,
                    COUNT(CASE WHEN o.status = 'completed' THEN 1 END) as completed_orders,
                    COALESCE(SUM(CASE WHEN o.status = 'completed' THEN o.amount END), 0) as total_revenue,
                    COALESCE(AVG(CASE WHEN o.status = 'completed' THEN o.amount END), 0) as avg_order_value,
                    MAX(o.order_date) as last_order_date,
                    DATEDIFF('day', MAX(o.order_date), CURRENT_DATE) as days_since_last_order
                FROM customers c
                LEFT JOIN orders o ON c.customer_id = o.customer_id
                GROUP BY c.customer_id, c.name, c.email, c.country, c.subscription_tier, c.signup_date
            """)
            transformation_result["models_created"].append("customer_analytics")
            
            # Create product analytics model
            conn.execute("""
                CREATE OR REPLACE TABLE product_analytics AS
                SELECT 
                    p.product_id,
                    p.name,
                    p.category,
                    p.price,
                    COUNT(o.order_id) as order_count,
                    SUM(o.quantity) as total_quantity_sold,
                    SUM(CASE WHEN o.status = 'completed' THEN o.amount ELSE 0 END) as total_revenue,
                    AVG(o.amount) as avg_order_amount,
                    p.stock_quantity,
                    CASE 
                        WHEN COUNT(o.order_id) = 0 THEN 'No Sales'
                        WHEN COUNT(o.order_id) < 5 THEN 'Low Sales'
                        WHEN COUNT(o.order_id) < 20 THEN 'Medium Sales'
                        ELSE 'High Sales'
                    END as sales_category
                FROM products p
                LEFT JOIN orders o ON p.category = o.product_category AND o.status = 'completed'
                GROUP BY p.product_id, p.name, p.category, p.price, p.stock_quantity
            """)
            transformation_result["models_created"].append("product_analytics")
            
            # Create business KPIs model
            conn.execute("""
                CREATE OR REPLACE TABLE business_kpis AS
                SELECT 
                    'overall' as metric_scope,
                    COUNT(DISTINCT c.customer_id) as total_customers,
                    COUNT(DISTINCT CASE WHEN o.order_date >= CURRENT_DATE - INTERVAL '30 days' THEN c.customer_id END) as active_customers_30d,
                    COUNT(o.order_id) as total_orders,
                    COUNT(CASE WHEN o.status = 'completed' THEN 1 END) as completed_orders,
                    SUM(CASE WHEN o.status = 'completed' THEN o.amount ELSE 0 END) as total_revenue,
                    AVG(CASE WHEN o.status = 'completed' THEN o.amount END) as avg_order_value,
                    SUM(CASE WHEN o.status = 'completed' THEN o.amount ELSE 0 END) / COUNT(DISTINCT c.customer_id) as revenue_per_customer,
                    CURRENT_TIMESTAMP as calculated_at
                FROM customers c
                LEFT JOIN orders o ON c.customer_id = o.customer_id
            """)
            transformation_result["models_created"].append("business_kpis")
            
            # Quality checks
            for model in transformation_result["models_created"]:
                row_count = conn.execute(f"SELECT COUNT(*) FROM {model}").fetchone()[0]
                transformation_result["quality_metrics"][model] = {
                    "row_count": row_count,
                    "has_data": row_count > 0
                }
                print(f"  âœ… Created {model} with {row_count} rows")
            
            conn.close()
        
        return transformation_result
    
    def _execute_orchestration(self, transformation_result):
        """Simulate Dagster orchestration"""
        print("ðŸŽ­ Executing orchestration stage...")
        
        with self.measure_execution_time("orchestration_stage"):
            # Simulate Dagster asset execution and dependency management
            orchestration_result = {
                "assets_materialized": [],
                "dependency_graph": {},
                "execution_plan": {},
                "orchestration_time": time.time()
            }
            
            # Define asset dependencies
            asset_dependencies = {
                "raw_data_assets": ["customers", "orders", "products"],
                "analytics_assets": ["customer_analytics", "product_analytics"],
                "kpi_assets": ["business_kpis"],
                "report_assets": ["customer_dashboard", "executive_summary"]
            }
            
            # Simulate asset materialization
            for asset_group, assets in asset_dependencies.items():
                for asset in assets:
                    orchestration_result["assets_materialized"].append(asset)
                    print(f"  âœ… Materialized asset: {asset}")
            
            orchestration_result["dependency_graph"] = asset_dependencies
            orchestration_result["total_assets"] = len(orchestration_result["assets_materialized"])
        
        return orchestration_result
    
    def _execute_reporting(self, orchestration_result):
        """Execute Evidence.dev reporting"""
        print("ðŸ“Š Executing reporting stage...")
        
        with self.measure_execution_time("reporting_stage"):
            evidence_project = self.test_env['paths']['evidence_project']
            build_dir = evidence_project / 'build'
            build_dir.mkdir(exist_ok=True)
            
            reporting_result = {
                "reports_generated": [],
                "build_artifacts": [],
                "reporting_time": time.time()
            }
            
            # Generate customer dashboard
            customer_dashboard = self._generate_customer_dashboard(build_dir)
            reporting_result["reports_generated"].append("customer_dashboard")
            reporting_result["build_artifacts"].append(str(customer_dashboard))
            
            # Generate executive summary
            executive_summary = self._generate_executive_summary(build_dir)
            reporting_result["reports_generated"].append("executive_summary")
            reporting_result["build_artifacts"].append(str(executive_summary))
            
            # Generate data quality report
            quality_report = self._generate_quality_report(build_dir)
            reporting_result["reports_generated"].append("quality_report")
            reporting_result["build_artifacts"].append(str(quality_report))
            
            print(f"  âœ… Generated {len(reporting_result['reports_generated'])} reports")
        
        return reporting_result
    
    def _generate_customer_dashboard(self, build_dir: Path):
        """Generate customer analytics dashboard"""
        dashboard_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Customer Analytics Dashboard</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .metric {{ background: #f5f5f5; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .metric h3 {{ margin: 0 0 10px 0; color: #333; }}
        .metric .value {{ font-size: 24px; font-weight: bold; color: #007acc; }}
    </style>
</head>
<body>
    <h1>Customer Analytics Dashboard</h1>
    <p>Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="metric">
        <h3>Total Customers</h3>
        <div class="value">1,000</div>
    </div>
    
    <div class="metric">
        <h3>Total Orders</h3>
        <div class="value">3,000</div>
    </div>
    
    <div class="metric">
        <h3>Average Order Value</h3>
        <div class="value">$245.67</div>
    </div>
    
    <h2>Customer Segmentation</h2>
    <p>Customer distribution by subscription tier and performance metrics.</p>
    
    <h2>Recent Activity</h2>
    <p>Latest customer activities and trends from the data pipeline.</p>
</body>
</html>
"""
        
        dashboard_file = build_dir / "customer-dashboard.html"
        with open(dashboard_file, 'w') as f:
            f.write(dashboard_content)
        
        return dashboard_file
    
    def _generate_executive_summary(self, build_dir: Path):
        """Generate executive summary report"""
        summary_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Executive Summary</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .kpi {{ display: inline-block; background: #e8f4f8; padding: 20px; margin: 10px; border-radius: 8px; text-align: center; }}
        .kpi h2 {{ margin: 0; color: #2c3e50; }}
        .kpi .value {{ font-size: 36px; font-weight: bold; color: #27ae60; }}
    </style>
</head>
<body>
    <h1>Executive Summary</h1>
    <p>Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="kpi">
        <h2>Total Revenue</h2>
        <div class="value">$737K</div>
    </div>
    
    <div class="kpi">
        <h2>Active Customers</h2>
        <div class="value">856</div>
    </div>
    
    <div class="kpi">
        <h2>Completion Rate</h2>
        <div class="value">94.2%</div>
    </div>
    
    <h2>Key Insights</h2>
    <ul>
        <li>Customer acquisition has increased 15% over the last quarter</li>
        <li>Premium tier customers show 3x higher lifetime value</li>
        <li>Mobile orders represent 67% of all transactions</li>
        <li>Customer retention rate is 89% for active users</li>
    </ul>
    
    <h2>Recommendations</h2>
    <ul>
        <li>Focus marketing efforts on premium tier conversion</li>
        <li>Optimize mobile checkout experience</li>
        <li>Implement customer loyalty program</li>
        <li>Expand product categories with highest margins</li>
    </ul>
</body>
</html>
"""
        
        summary_file = build_dir / "executive-summary.html"
        with open(summary_file, 'w') as f:
            f.write(summary_content)
        
        return summary_file
    
    def _generate_quality_report(self, build_dir: Path):
        """Generate data quality report"""
        quality_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Data Quality Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .quality-check {{ background: #f9f9f9; border-left: 4px solid #4CAF50; padding: 10px; margin: 10px 0; }}
        .quality-check.warning {{ border-left-color: #ff9800; }}
        .quality-check.error {{ border-left-color: #f44336; }}
    </style>
</head>
<body>
    <h1>Data Quality Report</h1>
    <p>Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <h2>Pipeline Health Check</h2>
    
    <div class="quality-check">
        <h3>âœ… Data Ingestion</h3>
        <p>All source files successfully processed. 4,100 total records ingested.</p>
    </div>
    
    <div class="quality-check">
        <h3>âœ… Data Storage</h3>
        <p>DuckDB tables created successfully. All indexes applied correctly.</p>
    </div>
    
    <div class="quality-check">
        <h3>âœ… Data Transformation</h3>
        <p>Analytics models created successfully. No data quality issues detected.</p>
    </div>
    
    <div class="quality-check">
        <h3>âœ… Report Generation</h3>
        <p>All reports generated successfully. Build artifacts created.</p>
    </div>
    
    <h2>Data Quality Metrics</h2>
    <ul>
        <li>Data completeness: 99.8%</li>
        <li>Data accuracy: 100%</li>
        <li>Schema compliance: 100%</li>
        <li>Referential integrity: 100%</li>
    </ul>
    
    <h2>Performance Metrics</h2>
    <ul>
        <li>Pipeline execution time: Under 5 minutes</li>
        <li>Data freshness: Real-time</li>
        <li>System availability: 99.9%</li>
        <li>Error rate: 0.1%</li>
    </ul>
</body>
</html>
"""
        
        quality_file = build_dir / "quality-report.html"
        with open(quality_file, 'w') as f:
            f.write(quality_content)
        
        return quality_file
    
    def _track_pipeline_stage(self, stage_name: str, input_data: dict, output_data: dict):
        """Track pipeline stage execution"""
        stage_info = {
            'stage': stage_name,
            'start_time': time.time(),
            'input_summary': str(input_data)[:100] + '...' if len(str(input_data)) > 100 else str(input_data),
            'output_summary': str(output_data)[:100] + '...' if len(str(output_data)) > 100 else str(output_data),
            'success': True
        }
        
        self.pipeline_state['stages'].append(stage_info)
        self.pipeline_state['data_lineage'][stage_name] = {
            'input': input_data,
            'output': output_data
        }
        
        print(f"  ðŸ“‹ Tracked stage: {stage_name}")
    
    def _validate_pipeline_completion(self):
        """Validate that the complete pipeline executed successfully"""
        expected_stages = [
            'data_ingestion',
            'data_storage', 
            'data_transformation',
            'orchestration',
            'reporting'
        ]
        
        executed_stages = [stage['stage'] for stage in self.pipeline_state['stages']]
        
        # Verify all stages executed
        for expected_stage in expected_stages:
            self.assertIn(expected_stage, executed_stages, 
                         f"Pipeline stage {expected_stage} was not executed")
        
        # Verify all stages succeeded
        for stage in self.pipeline_state['stages']:
            self.assertTrue(stage['success'], 
                           f"Pipeline stage {stage['stage']} failed")
        
        # Verify data lineage continuity
        self.assertEqual(len(self.pipeline_state['data_lineage']), len(expected_stages),
                        "Data lineage tracking incomplete")
        
        print("âœ… Pipeline validation completed successfully")
    
    def test_pipeline_error_recovery(self):
        """Test pipeline error recovery and rollback capabilities"""
        print("\nðŸ”§ Testing pipeline error recovery...")
        
        # Test 1: Simulate data ingestion failure
        with self.assertRaises(FileNotFoundError):
            # Try to load non-existent file
            import duckdb
            conn = duckdb.connect(":memory:")
            conn.execute("CREATE TABLE test AS SELECT * FROM read_csv_auto('/nonexistent/file.csv')")
        
        # Test 2: Simulate transformation failure
        with self.assertRaises(Exception):
            conn = duckdb.connect(":memory:")
            # Reference non-existent table
            conn.execute("SELECT * FROM nonexistent_table")
        
        # Test 3: Recovery mechanism simulation
        recovery_successful = self._simulate_error_recovery()
        self.assertTrue(recovery_successful, "Error recovery simulation failed")
        
        print("âœ… Error recovery tests completed")
    
    def _simulate_error_recovery(self):
        """Simulate error recovery mechanism"""
        try:
            # Simulate retry logic
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    # Simulate operation that might fail
                    if attempt < 2:  # Fail first 2 attempts
                        raise Exception(f"Simulated failure on attempt {attempt + 1}")
                    else:
                        # Success on 3rd attempt
                        return True
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    time.sleep(0.1)  # Brief retry delay
            
        except Exception:
            return False
        
        return True
    
    def test_pipeline_scalability(self):
        """Test pipeline performance with larger datasets"""
        print("\nðŸ“ˆ Testing pipeline scalability...")
        
        # Create larger dataset for scalability testing
        large_data = self._create_large_test_dataset()
        
        with self.measure_execution_time("large_dataset_pipeline"):
            # Execute abbreviated pipeline with large dataset
            storage_result = self._execute_data_storage(large_data)
            transformation_result = self._execute_data_transformation(storage_result)
        
        # Verify large dataset handling
        self.assertGreater(
            sum(storage_result["row_counts"].values()), 
            10000, 
            "Large dataset not processed correctly"
        )
        
        # Performance assertions for large dataset
        self.assert_performance_within_limits(
            self.performance_metrics["large_dataset_pipeline"],
            180.0,  # 3 minutes for large dataset
            "Large dataset pipeline"
        )
        
        print("âœ… Scalability tests completed")
    
    def _create_large_test_dataset(self):
        """Create large test dataset for scalability testing"""
        temp_dir = self.create_temp_directory("large_dataset_")
        
        # Generate 10,000 customers
        customers_file = temp_dir / "large_customers.csv"
        self._generate_realistic_customers(customers_file, 10000)
        
        # Generate 50,000 orders
        orders_file = temp_dir / "large_orders.csv"
        self._generate_realistic_orders(orders_file, 50000)
        
        # Generate 1,000 products
        products_file = temp_dir / "large_products.csv"
        self._generate_realistic_products(products_file, 1000)
        
        return {
            "extraction_time": time.time(),
            "source_system": "large_test_erp",
            "extracted_files": {
                "customers": str(customers_file),
                "orders": str(orders_file),
                "products": str(products_file)
            },
            "record_counts": {
                "customers": 10000,
                "orders": 50000,
                "products": 1000
            }
        }

if __name__ == "__main__":
    unittest.main(verbosity=2)
EOF

chmod +x tests/integration/test_complete_pipeline_integration.py

echo "âœ… Complete pipeline integration tests created"
```

### Configuration Compatibility Tests

#### Multi-Tool Configuration Validation

```bash
# Create configuration compatibility tests
cat > tests/integration/test_configuration_compatibility.py << 'EOF'
#!/usr/bin/env python3
"""
Configuration Compatibility Tests
Tests configuration consistency and compatibility across all tools in the stack
"""

import unittest
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'utilities'))

from base_test_framework import DataPractitionerIntegrationTest
from test_environment_setup import TestEnvironment
import yaml
import json
import configparser
from pathlib import Path

class ConfigurationCompatibilityTest(DataPractitionerIntegrationTest):
    """Tests for configuration compatibility across tools"""
    
    @classmethod
    def setUpClass(cls):
        """Set up test environment"""
        super().setUpClass()
        cls.test_env = TestEnvironment('integration').__enter__()
        
    @classmethod
    def tearDownClass(cls):
        """Clean up test environment"""
        TestEnvironment('integration').__exit__(None, None, None)
        super().tearDownClass()
    
    def setUp(self):
        """Set up individual test"""
        super().setUp()
        self.config_dir = self.test_env['paths']['config']
        self.db_path = self.test_env['databases']['integration_test_db']
        
    def test_database_connection_consistency(self):
        """Test that all tools use consistent database connections"""
        # DuckDB connection parameters
        base_connection = {
            'type': 'duckdb',
            'database': str(self.db_path),
            'schema': 'main'
        }
        
        # Test SQLmesh configuration
        sqlmesh_config = self._create_sqlmesh_config(base_connection)
        self._validate_sqlmesh_config(sqlmesh_config, base_connection)
        
        # Test dbt configuration
        dbt_config = self._create_dbt_config(base_connection)
        self._validate_dbt_config(dbt_config, base_connection)
        
        # Test Dagster configuration
        dagster_config = self._create_dagster_config(base_connection)
        self._validate_dagster_config(dagster_config, base_connection)
        
        # Test Evidence.dev configuration
        evidence_config = self._create_evidence_config(base_connection)
        self._validate_evidence_config(evidence_config, base_connection)
        
        print("âœ… Database connection consistency validated")
    
    def _create_sqlmesh_config(self, base_connection):
        """Create SQLmesh configuration"""
        config = {
            'gateways': {
                'local': {
                    'connection': {
                        'type': base_connection['type'],
                        'database': base_connection['database']
                    }
                }
            },
            'default_gateway': 'local',
            'model_defaults': {
                'dialect': base_connection['type']
            }
        }
        
        config_file = self.config_dir / 'sqlmesh_config.yaml'
        with open(config_file, 'w') as f:
            yaml.dump(config, f)
        
        return config
    
    def _create_dbt_config(self, base_connection):
        """Create dbt configuration"""
        profiles_config = {
            'data_practitioner': {
                'target': 'dev',
                'outputs': {
                    'dev': {
                        'type': base_connection['type'],
                        'path': base_connection['database']
                    }
                }
            }
        }
        
        dbt_project_config = {
            'name': 'data_practitioner_test',
            'version': '1.0.0',
            'profile': 'data_practitioner',
            'model-paths': ['models'],
            'analysis-paths': ['analysis'],
            'test-paths': ['tests'],
            'seed-paths': ['data'],
            'macro-paths': ['macros'],
            'snapshot-paths': ['snapshots'],
            'target-path': 'target',
            'clean-targets': ['target', 'dbt_packages']
        }
        
        # Save profiles.yml
        profiles_file = self.config_dir / 'profiles.yml'
        with open(profiles_file, 'w') as f:
            yaml.dump(profiles_config, f)
        
        # Save dbt_project.yml
        project_file = self.config_dir / 'dbt_project.yml'
        with open(project_file, 'w') as f:
            yaml.dump(dbt_project_config, f)
        
        return {
            'profiles': profiles_config,
            'project': dbt_project_config
        }
    
    def _create_dagster_config(self, base_connection):
        """Create Dagster configuration"""
        config = {
            'resources': {
                'duckdb_connection': {
                    'config': {
                        'database_path': base_connection['database']
                    }
                }
            },
            'ops': {
                'duckdb_ops': {
                    'config': {
                        'connection_type': base_connection['type']
                    }
                }
            }
        }
        
        config_file = self.config_dir / 'dagster_config.yaml'
        with open(config_file, 'w') as f:
            yaml.dump(config, f)
        
        return config
    
    def _create_evidence_config(self, base_connection):
        """Create Evidence.dev configuration"""
        config = {
            'database': {
                'type': base_connection['type'],
                'filename': base_connection['database']
            },
            'build': {
                'output_dir': 'build',
                'format': 'html'
            }
        }
        
        config_file = self.config_dir / 'evidence.settings.json'
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        
        return config
    
    def _validate_sqlmesh_config(self, config, base_connection):
        """Validate SQLmesh configuration"""
        gateway = config['gateways']['local']['connection']
        self.assertEqual(gateway['type'], base_connection['type'])
        self.assertEqual(gateway['database'], base_connection['database'])
        self.assertEqual(config['model_defaults']['dialect'], base_connection['type'])
    
    def _validate_dbt_config(self, config, base_connection):
        """Validate dbt configuration"""
        output = config['profiles']['data_practitioner']['outputs']['dev']
        self.assertEqual(output['type'], base_connection['type'])
        self.assertEqual(output['path'], base_connection['database'])
    
    def _validate_dagster_config(self, config, base_connection):
        """Validate Dagster configuration"""
        resource = config['resources']['duckdb_connection']['config']
        self.assertEqual(resource['database_path'], base_connection['database'])
        
        ops_config = config['ops']['duckdb_ops']['config']
        self.assertEqual(ops_config['connection_type'], base_connection['type'])
    
    def _validate_evidence_config(self, config, base_connection):
        """Validate Evidence.dev configuration"""
        db_config = config['database']
        self.assertEqual(db_config['type'], base_connection['type'])
        self.assertEqual(db_config['filename'], base_connection['database'])
    
    def test_environment_variable_consistency(self):
        """Test environment variable consistency across tools"""
        # Define required environment variables
        required_env_vars = {
            'DUCKDB_DATABASE_PATH': str(self.db_path),
            'DAGSTER_HOME': str(self.test_env['paths']['temp_root'] / 'dagster'),
            'DBT_PROFILES_DIR': str(self.config_dir),
            'EVIDENCE_PROJECT_DIR': str(self.test_env['paths']['evidence_project'])
        }
        
        # Set environment variables
        for var_name, var_value in required_env_vars.items():
            os.environ[var_name] = var_value
        
        # Validate environment variables are accessible
        for var_name, expected_value in required_env_vars.items():
            actual_value = os.environ.get(var_name)
            self.assertEqual(actual_value, expected_value, 
                           f"Environment variable {var_name} not set correctly")
        
        print("âœ… Environment variable consistency validated")
    
    def test_port_configuration_conflicts(self):
        """Test for port conflicts between tools"""
        # Define default ports for each tool
        tool_ports = {
            'dagster_webserver': 3001,
            'evidence_dev_server': 3000,
            'duckdb_http_server': 8080,
            'test_mock_server': 8081
        }
        
        # Check for port conflicts
        port_assignments = {}
        conflicts = []
        
        for tool, port in tool_ports.items():
            if port in port_assignments:
                conflicts.append(f"Port {port} conflict: {tool} vs {port_assignments[port]}")
            else:
                port_assignments[port] = tool
        
        # Assert no conflicts
        self.assertEqual(len(conflicts), 0, f"Port conflicts detected: {conflicts}")
        
        # Test port availability (simplified check)
        import socket
        for tool, port in tool_ports.items():
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                result = s.connect_ex(('localhost', port))
                # Port should be available (connection should fail)
                self.assertNotEqual(result, 0, f"Port {port} for {tool} is already in use")
        
        print("âœ… Port configuration validated")
    
    def test_data_type_mapping_consistency(self):
        """Test data type mapping consistency across tools"""
        # Define data type mappings for each tool
        type_mappings = {
            'source_types': ['INTEGER', 'VARCHAR', 'DECIMAL', 'BOOLEAN', 'TIMESTAMP', 'JSON'],
            'duckdb_types': ['INTEGER', 'VARCHAR', 'DECIMAL', 'BOOLEAN', 'TIMESTAMP', 'JSON'],
            'sqlmesh_types': ['INTEGER', 'VARCHAR', 'DECIMAL', 'BOOLEAN', 'TIMESTAMP', 'JSON'],
            'dbt_types': ['INTEGER', 'VARCHAR', 'DECIMAL', 'BOOLEAN', 'TIMESTAMP', 'JSON'],
            'evidence_types': ['number', 'string', 'number', 'boolean', 'datetime', 'object']
        }
        
        # Test type mapping consistency
        for i, source_type in enumerate(type_mappings['source_types']):
            # Verify DuckDB supports the type
            self.assertIn(type_mappings['duckdb_types'][i], type_mappings['duckdb_types'])
            
            # Verify SQLmesh supports the type
            self.assertIn(type_mappings['sqlmesh_types'][i], type_mappings['sqlmesh_types'])
            
            # Verify dbt supports the type
            self.assertIn(type_mappings['dbt_types'][i], type_mappings['dbt_types'])
        
        print("âœ… Data type mapping consistency validated")
    
    def test_security_configuration_alignment(self):
        """Test security configuration alignment across tools"""
        # Define security requirements
        security_config = {
            'encryption_at_rest': True,
            'encryption_in_transit': True,
            'authentication_required': False,  # For test environment
            'authorization_enabled': False,    # For test environment
            'audit_logging': True
        }
        
        # Test DuckDB security configuration
        self._validate_duckdb_security(security_config)
        
        # Test Dagster security configuration
        self._validate_dagster_security(security_config)
        
        # Test Evidence.dev security configuration
        self._validate_evidence_security(security_config)
        
        print("âœ… Security configuration alignment validated")
    
    def _validate_duckdb_security(self, security_config):
        """Validate DuckDB security configuration"""
        # For file-based DuckDB, encryption at rest depends on file system
        # In production, this would involve encrypted storage
        
        # Test basic security settings
        import duckdb
        conn = duckdb.connect(self.db_path)
        
        # Verify database is accessible
        result = conn.execute("SELECT 1 as test").fetchone()
        self.assertEqual(result[0], 1, "DuckDB security test failed")
        
        conn.close()
    
    def _validate_dagster_security(self, security_config):
        """Validate Dagster security configuration"""
        # Create Dagster security configuration
        dagster_security = {
            'webserver': {
                'host': '127.0.0.1',  # Localhost only for security
                'port': 3001,
                'debug': False,       # Disable debug in production
            },
            'run_launcher': {
                'type': 'DefaultRunLauncher'
            }
        }
        
        # Validate security settings
        self.assertEqual(dagster_security['webserver']['host'], '127.0.0.1')
        self.assertFalse(dagster_security['webserver']['debug'])
    
    def _validate_evidence_security(self, security_config):
        """Validate Evidence.dev security configuration"""
        # Create Evidence.dev security configuration
        evidence_security = {
            'server': {
                'host': 'localhost',
                'cors_enabled': False,
                'debug_mode': False
            },
            'database': {
                'readonly': True  # Read-only access for reporting
            }
        }
        
        # Validate security settings
        self.assertEqual(evidence_security['server']['host'], 'localhost')
        self.assertFalse(evidence_security['server']['debug_mode'])
        self.assertTrue(evidence_security['database']['readonly'])
    
    def test_version_compatibility_matrix(self):
        """Test version compatibility across tools"""
        # Define version compatibility matrix
        compatibility_matrix = {
            'python': {
                'version': '3.11.x',
                'compatible_with': ['duckdb', 'sqlmesh', 'dagster', 'pyairbyte']
            },
            'duckdb': {
                'version': '1.1.x',
                'compatible_with': ['python>=3.10', 'sqlmesh>=0.50', 'evidence>=25.0']
            },
            'sqlmesh': {
                'version': '0.57.x',
                'compatible_with': ['duckdb>=1.0', 'python>=3.10']
            },
            'dagster': {
                'version': '1.8.x',
                'compatible_with': ['python>=3.8', 'duckdb>=0.9']
            },
            'evidence': {
                'version': '25.x',
                'compatible_with': ['duckdb>=1.0']
            }
        }
        
        # Validate version compatibility
        for tool, info in compatibility_matrix.items():
            self.assertIn('version', info, f"Version not specified for {tool}")
            self.assertIn('compatible_with', info, f"Compatibility not specified for {tool}")
            self.assertGreater(len(info['compatible_with']), 0, 
                             f"No compatibility information for {tool}")
        
        print("âœ… Version compatibility matrix validated")
    
    def test_logging_configuration_consistency(self):
        """Test logging configuration consistency"""
        # Define unified logging configuration
        logging_config = {
            'version': 1,
            'disable_existing_loggers': False,
            'formatters': {
                'standard': {
                    'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
                }
            },
            'handlers': {
                'console': {
                    'level': 'INFO',
                    'class': 'logging.StreamHandler',
                    'formatter': 'standard'
                },
                'file': {
                    'level': 'DEBUG',
                    'class': 'logging.FileHandler',
                    'filename': str(self.test_env['paths']['logs'] / 'data_practitioner.log'),
                    'formatter': 'standard'
                }
            },
            'loggers': {
                'duckdb': {'level': 'INFO', 'handlers': ['console', 'file']},
                'sqlmesh': {'level': 'INFO', 'handlers': ['console', 'file']},
                'dagster': {'level': 'INFO', 'handlers': ['console', 'file']},
                'evidence': {'level': 'INFO', 'handlers': ['console', 'file']}
            }
        }
        
        # Save logging configuration
        logging_config_file = self.config_dir / 'logging_config.yaml'
        with open(logging_config_file, 'w') as f:
            yaml.dump(logging_config, f)
        
        # Validate logging configuration
        self.assert_file_exists(str(logging_config_file), "Logging configuration file not created")
        
        # Test log directory exists
        log_dir = self.test_env['paths']['logs']
        self.assert_directory_exists(str(log_dir), "Log directory not created")
        
        print("âœ… Logging configuration consistency validated")

if __name__ == "__main__":
    unittest.main(verbosity=2)
EOF

chmod +x tests/integration/test_configuration_compatibility.py

echo "âœ… Configuration compatibility tests created"
```

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "Create Priority 2 Deliverable 1: Integration Testing Framework Foundation", "status": "completed", "id": "6"}, {"content": "Create Priority 2 Deliverable 2: Cross-Tool Testing Patterns", "status": "completed", "id": "7"}, {"content": "Create Priority 2 Deliverable 3: Agent Coordination Testing", "status": "in_progress", "id": "8"}, {"content": "Create Priority 2 Deliverable 4: Performance Testing Suite", "status": "pending", "id": "9"}, {"content": "Create Priority 3 Deliverable 1: Operations & Monitoring Documentation", "status": "pending", "id": "10"}]