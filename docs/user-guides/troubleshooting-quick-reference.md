# Advanced Troubleshooting Intelligence

Your comprehensive guide to diagnosing, fixing, and preventing issues in the Data Practitioner agent ecosystem.

## üß† Intelligent Error Recognition

### Error Pattern Classification System

**Priority Level Matrix:**
- **P0 - Critical**: System broken, no data flowing, agents unresponsive
- **P1 - High**: Degraded performance, data quality issues, partial failures
- **P2 - Medium**: Feature limitations, optimization opportunities, minor bugs
- **P3 - Low**: Enhancement requests, documentation gaps, cosmetic issues

**Error Type Categories:**
- **Environment**: Python/Node.js version conflicts, missing dependencies
- **Cross-Language**: Node.js ‚Üî Python subprocess communication failures
- **Agent**: Agent activation, command execution, state management
- **Data Pipeline**: PyAirbyte, DuckDB, SQLmesh/dbt, Dagster, Evidence.dev
- **Performance**: Memory leaks, slow queries, resource exhaustion
- **Integration**: Tool coordination, configuration conflicts, version mismatches

---

## üîß Cross-Language Debugging Techniques

### Node.js + Python Subprocess Issues

#### Issue 1: Python Virtual Environment Not Detected

**Symptoms:**
```bash
Error: python: command not found
ModuleNotFoundError: No module named 'duckdb'
```

**Root Cause Analysis:**
```bash
# Step 1: Verify virtual environment
which python
echo $VIRTUAL_ENV

# Step 2: Check subprocess PATH
node -e "console.log(process.env.PATH.split(':').filter(p => p.includes('venv')))"

# Step 3: Test subprocess communication
node -e "
const { spawn } = require('child_process');
const python = spawn('python', ['-c', 'import sys; print(sys.executable)']);
python.stdout.on('data', (data) => console.log('Python path:', data.toString()));
python.stderr.on('data', (data) => console.error('Error:', data.toString()));
"
```

**Advanced Resolution:**
```bash
# Create environment-aware subprocess helper
cat > bmad-method/utils/python-subprocess.js << 'EOF'
const { spawn } = require('child_process');
const path = require('path');

class PythonSubprocess {
    constructor(virtualEnvPath = null) {
        this.pythonPath = virtualEnvPath 
            ? path.join(virtualEnvPath, 'bin', 'python')
            : 'python';
        this.envVars = {
            ...process.env,
            PYTHONPATH: process.cwd(),
            VIRTUAL_ENV: virtualEnvPath
        };
    }

    async execute(command, args = [], options = {}) {
        return new Promise((resolve, reject) => {
            const python = spawn(this.pythonPath, ['-c', command, ...args], {
                env: this.envVars,
                ...options
            });

            let stdout = '';
            let stderr = '';

            python.stdout.on('data', (data) => stdout += data.toString());
            python.stderr.on('data', (data) => stderr += data.toString());

            python.on('close', (code) => {
                if (code === 0) {
                    resolve({ stdout, stderr, code });
                } else {
                    reject(new Error(`Python process failed with code ${code}: ${stderr}`));
                }
            });

            // Timeout protection
            setTimeout(() => {
                python.kill('SIGTERM');
                reject(new Error('Python process timeout'));
            }, options.timeout || 60000);
        });
    }
}

module.exports = PythonSubprocess;
EOF

echo "‚úÖ Environment-aware subprocess helper created"
```

#### Issue 2: Memory Leaks in Long-Running Processes

**Symptoms:**
```bash
ENOMEM: not enough memory
Process killed: signal 9 (SIGKILL)
```

**Memory Monitoring Solution:**
```bash
# Create memory monitoring utility
cat > bmad-method/utils/memory-monitor.js << 'EOF'
const os = require('os');

class MemoryMonitor {
    constructor(thresholds = { warning: 0.8, critical: 0.9 }) {
        this.thresholds = thresholds;
        this.isMonitoring = false;
        this.alerts = [];
    }

    startMonitoring(intervalMs = 5000) {
        if (this.isMonitoring) return;
        
        this.isMonitoring = true;
        this.interval = setInterval(() => {
            const usage = this.getMemoryUsage();
            this.checkThresholds(usage);
        }, intervalMs);
    }

    getMemoryUsage() {
        const totalMem = os.totalmem();
        const freeMem = os.freemem();
        const usedMem = totalMem - freeMem;
        const processUsage = process.memoryUsage();

        return {
            system: {
                total: totalMem,
                used: usedMem,
                free: freeMem,
                percentage: usedMem / totalMem
            },
            process: {
                rss: processUsage.rss,
                heapUsed: processUsage.heapUsed,
                heapTotal: processUsage.heapTotal,
                external: processUsage.external
            }
        };
    }

    checkThresholds(usage) {
        const systemPercent = usage.system.percentage;
        
        if (systemPercent > this.thresholds.critical) {
            this.alerts.push({
                level: 'CRITICAL',
                message: `System memory usage: ${(systemPercent * 100).toFixed(1)}%`,
                timestamp: new Date(),
                action: 'Consider restarting processes or increasing system memory'
            });
        } else if (systemPercent > this.thresholds.warning) {
            this.alerts.push({
                level: 'WARNING',
                message: `System memory usage: ${(systemPercent * 100).toFixed(1)}%`,
                timestamp: new Date(),
                action: 'Monitor closely, consider optimization'
            });
        }
    }

    getRecentAlerts(since = 300000) { // 5 minutes
        const cutoff = new Date(Date.now() - since);
        return this.alerts.filter(alert => alert.timestamp > cutoff);
    }

    stopMonitoring() {
        if (this.interval) {
            clearInterval(this.interval);
            this.isMonitoring = false;
        }
    }
}

module.exports = MemoryMonitor;
EOF

echo "‚úÖ Memory monitoring utility created"
```

#### Issue 3: JSON Communication Failures

**Symptoms:**
```bash
SyntaxError: Unexpected token in JSON
TypeError: Cannot read property 'data' of undefined
```

**Robust JSON Communication:**
```bash
# Create JSON communication wrapper
cat > bmad-method/utils/json-communicator.js << 'EOF'
class JSONCommunicator {
    static safeParse(data) {
        try {
            return { success: true, data: JSON.parse(data) };
        } catch (error) {
            return { 
                success: false, 
                error: error.message, 
                rawData: data.substring(0, 200) + '...' 
            };
        }
    }

    static safeStringify(obj, maxDepth = 10) {
        try {
            return JSON.stringify(obj, this.createCircularReplacer(), 2);
        } catch (error) {
            return JSON.stringify({ 
                error: 'Serialization failed', 
                message: error.message,
                type: typeof obj 
            });
        }
    }

    static createCircularReplacer() {
        const seen = new WeakSet();
        return (key, value) => {
            if (typeof value === "object" && value !== null) {
                if (seen.has(value)) {
                    return "[Circular Reference]";
                }
                seen.add(value);
            }
            return value;
        };
    }

    static validateStructure(data, schema) {
        const errors = [];
        
        for (const [key, expectedType] of Object.entries(schema)) {
            if (!(key in data)) {
                errors.push(`Missing required field: ${key}`);
            } else if (typeof data[key] !== expectedType) {
                errors.push(`Field ${key} should be ${expectedType}, got ${typeof data[key]}`);
            }
        }

        return {
            isValid: errors.length === 0,
            errors
        };
    }
}

module.exports = JSONCommunicator;
EOF

echo "‚úÖ JSON communication wrapper created"
```

---

## üóÑÔ∏è Data Tool Specific Diagnostics

### DuckDB Issues

#### Issue 1: Database Lock Conflicts

**Symptoms:**
```bash
database is locked
IO Error: Could not open file
```

**Diagnostic Commands:**
```bash
# Check active connections
python -c "
import duckdb
import os

db_path = 'data/analytics.duckdb'
if os.path.exists(db_path + '.wal'):
    print('‚ö†Ô∏è  WAL file exists - database may be locked')

try:
    conn = duckdb.connect(db_path)
    result = conn.execute('PRAGMA database_list').fetchall()
    print('‚úÖ Database accessible')
    conn.close()
except Exception as e:
    print(f'‚ùå Database error: {e}')
"

# Force unlock if needed
rm -f data/analytics.duckdb.wal
rm -f data/analytics.duckdb-shm
```

**Prevention Strategy:**
```python
# Connection pool with proper cleanup
class DuckDBConnectionManager:
    def __init__(self, db_path, max_connections=5):
        self.db_path = db_path
        self.max_connections = max_connections
        self.active_connections = {}
        
    def get_connection(self, connection_id=None):
        if connection_id is None:
            connection_id = f"conn_{len(self.active_connections)}"
            
        if connection_id not in self.active_connections:
            if len(self.active_connections) >= self.max_connections:
                # Close oldest connection
                oldest = min(self.active_connections.keys())
                self.close_connection(oldest)
                
            self.active_connections[connection_id] = duckdb.connect(self.db_path)
            
        return self.active_connections[connection_id]
    
    def close_connection(self, connection_id):
        if connection_id in self.active_connections:
            self.active_connections[connection_id].close()
            del self.active_connections[connection_id]
            
    def close_all(self):
        for conn_id in list(self.active_connections.keys()):
            self.close_connection(conn_id)
```

#### Issue 2: Memory Configuration Problems

**Symptoms:**
```bash
Out of Memory Error
std::bad_alloc
```

**Memory Optimization:**
```bash
# Test optimal memory settings
python -c "
import duckdb

conn = duckdb.connect(':memory:')

# Test current limits
try:
    conn.execute('SET memory_limit = \"512MB\"')
    conn.execute('SET max_memory = \"1GB\"')
    conn.execute('SET threads = 2')
    print('‚úÖ Memory configuration applied')
    
    # Test with sample data
    conn.execute('CREATE TABLE test AS SELECT range as id FROM range(100000)')
    result = conn.execute('SELECT COUNT(*) FROM test').fetchone()
    print(f'‚úÖ Memory test passed: {result[0]} rows')
    
except Exception as e:
    print(f'‚ùå Memory configuration failed: {e}')
    
conn.close()
"
```

### SQLmesh vs dbt Coordination Issues

#### Issue 1: Engine Selection Conflicts

**Symptoms:**
```bash
Multiple transformation engines detected
Configuration conflict between dbt and SQLmesh
```

**Auto-Detection Diagnostic:**
```bash
# Create engine detection utility
cat > bmad-method/utils/engine-detector.js << 'EOF'
const fs = require('fs');
const path = require('path');

class TransformationEngineDetector {
    static detectEngines(projectPath = '.') {
        const indicators = {
            dbt: {
                files: ['dbt_project.yml', 'profiles.yml'],
                directories: ['models', 'macros', 'tests'],
                score: 0
            },
            sqlmesh: {
                files: ['config.yaml', 'config.yml'],
                directories: ['models', 'audits', 'tests'],
                score: 0
            }
        };

        // Check for indicator files and directories
        for (const [engine, config] of Object.entries(indicators)) {
            config.files.forEach(file => {
                if (fs.existsSync(path.join(projectPath, file))) {
                    config.score += 2;
                }
            });

            config.directories.forEach(dir => {
                if (fs.existsSync(path.join(projectPath, dir))) {
                    config.score += 1;
                }
            });
        }

        // Determine recommendation
        const recommendation = this.getRecommendation(indicators);
        
        return {
            detected: indicators,
            recommendation,
            hasConflict: indicators.dbt.score > 0 && indicators.sqlmesh.score > 0
        };
    }

    static getRecommendation(indicators) {
        const dbtScore = indicators.dbt.score;
        const sqlmeshScore = indicators.sqlmesh.score;

        if (dbtScore === 0 && sqlmeshScore === 0) {
            return { engine: 'none', confidence: 'high', reason: 'No transformation engine detected' };
        }

        if (dbtScore > sqlmeshScore) {
            return { 
                engine: 'dbt', 
                confidence: dbtScore > 3 ? 'high' : 'medium',
                reason: 'dbt indicators dominant'
            };
        }

        if (sqlmeshScore > dbtScore) {
            return { 
                engine: 'sqlmesh', 
                confidence: sqlmeshScore > 3 ? 'high' : 'medium',
                reason: 'SQLmesh indicators dominant'
            };
        }

        return { 
            engine: 'hybrid', 
            confidence: 'low',
            reason: 'Both engines detected - manual selection required'
        };
    }
}

module.exports = TransformationEngineDetector;
EOF

echo "‚úÖ Engine detection utility created"
```

#### Issue 2: Configuration Conflicts

**Resolution Strategy:**
```bash
# Create configuration validator
cat > bmad-method/utils/config-validator.js << 'EOF'
const fs = require('fs');
const yaml = require('js-yaml');

class ConfigurationValidator {
    static validateDataStackConfig(configPath) {
        const errors = [];
        const warnings = [];

        try {
            const config = yaml.load(fs.readFileSync(configPath, 'utf8'));

            // Validate transformation engine selection
            const engines = config.transformation_engines || [];
            if (engines.length === 0) {
                errors.push('No transformation engines configured');
            } else if (engines.length > 1) {
                warnings.push('Multiple transformation engines configured - ensure proper coordination');
            }

            // Validate database connections
            if (!config.database || !config.database.connection) {
                errors.push('Database connection not configured');
            }

            // Validate Python environment
            if (!config.python || !config.python.executable) {
                warnings.push('Python executable path not specified');
            }

            // Check for port conflicts
            const ports = this.extractPorts(config);
            const duplicates = this.findDuplicatePorts(ports);
            if (duplicates.length > 0) {
                errors.push(`Port conflicts detected: ${duplicates.join(', ')}`);
            }

            return {
                isValid: errors.length === 0,
                errors,
                warnings,
                config
            };

        } catch (error) {
            return {
                isValid: false,
                errors: [`Configuration file error: ${error.message}`],
                warnings: [],
                config: null
            };
        }
    }

    static extractPorts(config) {
        const ports = [];
        
        if (config.dagster && config.dagster.webserver_port) {
            ports.push({ service: 'dagster', port: config.dagster.webserver_port });
        }
        
        if (config.evidence && config.evidence.port) {
            ports.push({ service: 'evidence', port: config.evidence.port });
        }

        return ports;
    }

    static findDuplicatePorts(ports) {
        const portCounts = {};
        const duplicates = [];

        ports.forEach(({ service, port }) => {
            if (portCounts[port]) {
                duplicates.push(`${port} (${portCounts[port]} vs ${service})`);
            } else {
                portCounts[port] = service;
            }
        });

        return duplicates;
    }
}

module.exports = ConfigurationValidator;
EOF

echo "‚úÖ Configuration validator created"
```

---

## ‚ö° Performance Bottleneck Identification

### Query Performance Analysis

#### Slow DuckDB Queries

**Performance Profiling:**
```bash
# Create query profiler
cat > bmad-method/utils/query-profiler.py << 'EOF'
import duckdb
import time
import json
from typing import Dict, Any

class QueryProfiler:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.profile_results = []
    
    def profile_query(self, query: str, query_name: str = None) -> Dict[str, Any]:
        """Profile a single query execution"""
        conn = duckdb.connect(self.db_path)
        
        try:
            # Enable profiling
            conn.execute("PRAGMA enable_profiling")
            conn.execute("PRAGMA profiling_mode = 'detailed'")
            
            start_time = time.time()
            result = conn.execute(query)
            rows = result.fetchall()
            end_time = time.time()
            
            # Get profile info
            profile_info = conn.execute("PRAGMA profile_output").fetchone()
            
            profile_result = {
                'query_name': query_name or f'query_{len(self.profile_results)}',
                'execution_time': end_time - start_time,
                'row_count': len(rows),
                'profile_info': profile_info[0] if profile_info else None,
                'timestamp': time.time()
            }
            
            self.profile_results.append(profile_result)
            return profile_result
            
        except Exception as e:
            return {
                'query_name': query_name,
                'error': str(e),
                'timestamp': time.time()
            }
        finally:
            conn.close()
    
    def identify_slow_queries(self, threshold_seconds: float = 1.0):
        """Identify queries exceeding performance threshold"""
        slow_queries = [
            result for result in self.profile_results 
            if result.get('execution_time', 0) > threshold_seconds
        ]
        
        return sorted(slow_queries, key=lambda x: x.get('execution_time', 0), reverse=True)
    
    def generate_optimization_report(self):
        """Generate performance optimization recommendations"""
        slow_queries = self.identify_slow_queries()
        
        report = {
            'total_queries': len(self.profile_results),
            'slow_queries': len(slow_queries),
            'recommendations': []
        }
        
        for query in slow_queries[:5]:  # Top 5 slow queries
            recommendations = []
            
            if query.get('execution_time', 0) > 5:
                recommendations.append('Consider adding indexes or partitioning')
            
            if query.get('row_count', 0) > 1000000:
                recommendations.append('Large result set - consider LIMIT or pagination')
                
            recommendations.append('Review query plan for optimization opportunities')
            
            report['recommendations'].append({
                'query_name': query['query_name'],
                'execution_time': query['execution_time'],
                'suggestions': recommendations
            })
        
        return report

if __name__ == "__main__":
    profiler = QueryProfiler("data/analytics.duckdb")
    
    # Example usage
    test_queries = [
        ("SELECT COUNT(*) FROM customers", "customer_count"),
        ("SELECT * FROM orders ORDER BY order_date", "orders_by_date")
    ]
    
    for query, name in test_queries:
        result = profiler.profile_query(query, name)
        print(f"‚úÖ Profiled {name}: {result.get('execution_time', 'ERROR'):.3f}s")
    
    report = profiler.generate_optimization_report()
    print("\nüìä Performance Report:")
    print(json.dumps(report, indent=2))
EOF

echo "‚úÖ Query profiler created"
```

#### Memory and Resource Management

**Resource Monitor:**
```bash
# Create comprehensive resource monitor
cat > bmad-method/utils/resource-monitor.py << 'EOF'
import psutil
import time
import json
import threading
from typing import Dict, List, Any

class ResourceMonitor:
    def __init__(self, monitoring_interval: int = 5):
        self.monitoring_interval = monitoring_interval
        self.is_monitoring = False
        self.metrics_history = []
        self.alerts = []
        self.thresholds = {
            'cpu_percent': 80.0,
            'memory_percent': 85.0,
            'disk_usage_percent': 90.0
        }
    
    def start_monitoring(self):
        """Start continuous resource monitoring"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """Stop resource monitoring"""
        self.is_monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=1)
    
    def _monitor_loop(self):
        """Main monitoring loop"""
        while self.is_monitoring:
            metrics = self.collect_metrics()
            self.metrics_history.append(metrics)
            self.check_thresholds(metrics)
            
            # Keep only last 1000 records
            if len(self.metrics_history) > 1000:
                self.metrics_history = self.metrics_history[-1000:]
            
            time.sleep(self.monitoring_interval)
    
    def collect_metrics(self) -> Dict[str, Any]:
        """Collect current system metrics"""
        try:
            return {
                'timestamp': time.time(),
                'cpu': {
                    'percent': psutil.cpu_percent(interval=1),
                    'count': psutil.cpu_count(),
                    'load_avg': psutil.getloadavg() if hasattr(psutil, 'getloadavg') else None
                },
                'memory': {
                    'percent': psutil.virtual_memory().percent,
                    'available': psutil.virtual_memory().available,
                    'total': psutil.virtual_memory().total,
                    'used': psutil.virtual_memory().used
                },
                'disk': {
                    'percent': psutil.disk_usage('/').percent,
                    'free': psutil.disk_usage('/').free,
                    'total': psutil.disk_usage('/').total
                },
                'processes': {
                    'total': len(psutil.pids()),
                    'python_processes': len([p for p in psutil.process_iter(['name']) if 'python' in p.info['name'].lower()]),
                    'node_processes': len([p for p in psutil.process_iter(['name']) if 'node' in p.info['name'].lower()])
                }
            }
        except Exception as e:
            return {
                'timestamp': time.time(),
                'error': str(e)
            }
    
    def check_thresholds(self, metrics: Dict[str, Any]):
        """Check if metrics exceed thresholds"""
        if 'error' in metrics:
            return
        
        alerts = []
        
        if metrics['cpu']['percent'] > self.thresholds['cpu_percent']:
            alerts.append({
                'type': 'CPU_HIGH',
                'value': metrics['cpu']['percent'],
                'threshold': self.thresholds['cpu_percent'],
                'timestamp': metrics['timestamp']
            })
        
        if metrics['memory']['percent'] > self.thresholds['memory_percent']:
            alerts.append({
                'type': 'MEMORY_HIGH',
                'value': metrics['memory']['percent'],
                'threshold': self.thresholds['memory_percent'],
                'timestamp': metrics['timestamp']
            })
        
        if metrics['disk']['percent'] > self.thresholds['disk_usage_percent']:
            alerts.append({
                'type': 'DISK_HIGH',
                'value': metrics['disk']['percent'],
                'threshold': self.thresholds['disk_usage_percent'],
                'timestamp': metrics['timestamp']
            })
        
        self.alerts.extend(alerts)
    
    def get_recent_alerts(self, minutes: int = 10) -> List[Dict[str, Any]]:
        """Get alerts from the last N minutes"""
        cutoff_time = time.time() - (minutes * 60)
        return [alert for alert in self.alerts if alert['timestamp'] > cutoff_time]
    
    def generate_health_report(self) -> Dict[str, Any]:
        """Generate system health report"""
        if not self.metrics_history:
            return {'error': 'No metrics collected yet'}
        
        latest = self.metrics_history[-1]
        recent_alerts = self.get_recent_alerts()
        
        # Calculate averages over last 10 minutes
        recent_metrics = [m for m in self.metrics_history if m.get('timestamp', 0) > time.time() - 600]
        
        if recent_metrics:
            avg_cpu = sum(m.get('cpu', {}).get('percent', 0) for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m.get('memory', {}).get('percent', 0) for m in recent_metrics) / len(recent_metrics)
        else:
            avg_cpu = avg_memory = 0
        
        return {
            'status': 'HEALTHY' if len(recent_alerts) == 0 else 'WARNING' if len(recent_alerts) < 5 else 'CRITICAL',
            'current': latest,
            'averages': {
                'cpu_percent': avg_cpu,
                'memory_percent': avg_memory
            },
            'recent_alerts': recent_alerts,
            'recommendations': self._generate_recommendations(latest, recent_alerts)
        }
    
    def _generate_recommendations(self, latest_metrics: Dict[str, Any], recent_alerts: List[Dict[str, Any]]) -> List[str]:
        """Generate optimization recommendations"""
        recommendations = []
        
        if any(alert['type'] == 'CPU_HIGH' for alert in recent_alerts):
            recommendations.append('High CPU usage detected - consider reducing concurrent operations')
        
        if any(alert['type'] == 'MEMORY_HIGH' for alert in recent_alerts):
            recommendations.append('High memory usage - check for memory leaks or reduce data processing batch sizes')
        
        if any(alert['type'] == 'DISK_HIGH' for alert in recent_alerts):
            recommendations.append('Disk space low - clean up temporary files and old data')
        
        if latest_metrics.get('processes', {}).get('python_processes', 0) > 10:
            recommendations.append('Many Python processes running - consider connection pooling')
        
        return recommendations

if __name__ == "__main__":
    monitor = ResourceMonitor(monitoring_interval=2)
    
    print("üöÄ Starting resource monitoring...")
    monitor.start_monitoring()
    
    try:
        time.sleep(30)  # Monitor for 30 seconds
        
        health_report = monitor.generate_health_report()
        print("\nüìä Health Report:")
        print(json.dumps(health_report, indent=2, default=str))
        
    finally:
        monitor.stop_monitoring()
EOF

echo "‚úÖ Resource monitor created"
```

---

## üîÑ Recovery Procedures

### Automated Recovery Scripts

#### Database Recovery

**DuckDB Recovery:**
```bash
# Create database recovery utility
cat > bmad-method/utils/database-recovery.py << 'EOF'
import duckdb
import os
import shutil
import time
from pathlib import Path

class DatabaseRecovery:
    def __init__(self, db_path: str, backup_dir: str = "backups"):
        self.db_path = db_path
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(exist_ok=True)
    
    def create_backup(self, backup_name: str = None) -> str:
        """Create database backup"""
        if not os.path.exists(self.db_path):
            raise FileNotFoundError(f"Database not found: {self.db_path}")
        
        if backup_name is None:
            backup_name = f"backup_{int(time.time())}.duckdb"
        
        backup_path = self.backup_dir / backup_name
        shutil.copy2(self.db_path, backup_path)
        
        return str(backup_path)
    
    def restore_from_backup(self, backup_path: str):
        """Restore database from backup"""
        if not os.path.exists(backup_path):
            raise FileNotFoundError(f"Backup not found: {backup_path}")
        
        # Create current backup before restore
        if os.path.exists(self.db_path):
            emergency_backup = self.create_backup(f"emergency_backup_{int(time.time())}")
            print(f"Created emergency backup: {emergency_backup}")
        
        shutil.copy2(backup_path, self.db_path)
        print(f"Database restored from: {backup_path}")
    
    def repair_database(self) -> bool:
        """Attempt to repair corrupted database"""
        try:
            # Test connection
            conn = duckdb.connect(self.db_path)
            conn.execute("SELECT 1").fetchone()
            conn.close()
            return True
        except Exception as e:
            print(f"Database corruption detected: {e}")
            
            # Attempt repair by exporting and reimporting
            try:
                return self._export_import_repair()
            except Exception as repair_error:
                print(f"Repair failed: {repair_error}")
                return False
    
    def _export_import_repair(self) -> bool:
        """Repair by exporting data and creating new database"""
        temp_dir = Path("temp_repair")
        temp_dir.mkdir(exist_ok=True)
        
        try:
            # Connect to corrupted database in read-only mode
            conn = duckdb.connect(self.db_path, read_only=True)
            
            # Get list of tables
            tables = conn.execute("SHOW TABLES").fetchall()
            
            # Export each table to CSV
            for table_row in tables:
                table_name = table_row[0]
                csv_path = temp_dir / f"{table_name}.csv"
                conn.execute(f"COPY {table_name} TO '{csv_path}' (FORMAT CSV, HEADER)")
            
            conn.close()
            
            # Create new database
            new_db_path = f"{self.db_path}.repaired"
            new_conn = duckdb.connect(new_db_path)
            
            # Import tables back
            for table_row in tables:
                table_name = table_row[0]
                csv_path = temp_dir / f"{table_name}.csv"
                new_conn.execute(f"CREATE TABLE {table_name} AS SELECT * FROM read_csv_auto('{csv_path}')")
            
            new_conn.close()
            
            # Replace original with repaired
            shutil.move(self.db_path, f"{self.db_path}.corrupted")
            shutil.move(new_db_path, self.db_path)
            
            return True
            
        finally:
            # Cleanup
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
    
    def list_backups(self):
        """List available backups"""
        backups = list(self.backup_dir.glob("*.duckdb"))
        backups.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        return [{
            'name': backup.name,
            'path': str(backup),
            'size': backup.stat().st_size,
            'created': time.ctime(backup.stat().st_mtime)
        } for backup in backups]

if __name__ == "__main__":
    recovery = DatabaseRecovery("data/analytics.duckdb")
    
    # Test database health
    if recovery.repair_database():
        print("‚úÖ Database is healthy")
    else:
        print("‚ùå Database repair failed")
    
    # List available backups
    backups = recovery.list_backups()
    print(f"\nüìã Available backups: {len(backups)}")
    for backup in backups[:3]:  # Show latest 3
        print(f"  - {backup['name']} ({backup['created']})")
EOF

echo "‚úÖ Database recovery utility created"
```

#### Agent State Recovery

**Agent Recovery:**
```bash
# Create agent recovery system
cat > bmad-method/utils/agent-recovery.js << 'EOF'
const fs = require('fs');
const path = require('path');

class AgentRecovery {
    constructor(stateDir = '.bmad-states') {
        this.stateDir = stateDir;
        this.ensureStateDirectory();
    }

    ensureStateDirectory() {
        if (!fs.existsSync(this.stateDir)) {
            fs.mkdirSync(this.stateDir, { recursive: true });
        }
    }

    saveAgentState(agentId, state) {
        const statePath = path.join(this.stateDir, `${agentId}.json`);
        const stateData = {
            agentId,
            state,
            timestamp: Date.now(),
            version: '1.0'
        };

        try {
            fs.writeFileSync(statePath, JSON.stringify(stateData, null, 2));
            return true;
        } catch (error) {
            console.error(`Failed to save agent state: ${error.message}`);
            return false;
        }
    }

    loadAgentState(agentId) {
        const statePath = path.join(this.stateDir, `${agentId}.json`);
        
        if (!fs.existsSync(statePath)) {
            return null;
        }

        try {
            const stateData = JSON.parse(fs.readFileSync(statePath, 'utf8'));
            
            // Validate state age (don't load states older than 1 hour)
            const maxAge = 60 * 60 * 1000; // 1 hour
            if (Date.now() - stateData.timestamp > maxAge) {
                console.warn(`Agent state for ${agentId} is stale, ignoring`);
                return null;
            }

            return stateData.state;
        } catch (error) {
            console.error(`Failed to load agent state: ${error.message}`);
            return null;
        }
    }

    recoverAgent(agentId) {
        const state = this.loadAgentState(agentId);
        
        if (!state) {
            console.log(`No recoverable state for agent ${agentId}`);
            return this.getDefaultAgentState(agentId);
        }

        console.log(`Recovering agent ${agentId} from saved state`);
        return state;
    }

    getDefaultAgentState(agentId) {
        // Default states for each agent type
        const defaults = {
            'data-engineer': {
                activePipelines: [],
                configuration: {},
                lastHealthCheck: null
            },
            'data-analyst': {
                activeQueries: [],
                savedViews: [],
                currentContext: null
            },
            'data-architect': {
                activeDesigns: [],
                costOptimizations: [],
                architectureReviews: []
            }
        };

        return defaults[agentId] || {
            initialized: false,
            lastActivity: null
        };
    }

    healthCheck() {
        const stateFiles = fs.readdirSync(this.stateDir)
            .filter(file => file.endsWith('.json'));

        const health = {
            totalAgents: stateFiles.length,
            activeAgents: 0,
            staleAgents: 0,
            corruptedStates: 0,
            details: []
        };

        const maxAge = 60 * 60 * 1000; // 1 hour

        stateFiles.forEach(file => {
            const agentId = path.basename(file, '.json');
            const statePath = path.join(this.stateDir, file);

            try {
                const stateData = JSON.parse(fs.readFileSync(statePath, 'utf8'));
                const age = Date.now() - stateData.timestamp;

                if (age > maxAge) {
                    health.staleAgents++;
                    health.details.push({
                        agentId,
                        status: 'stale',
                        age: Math.round(age / 1000 / 60) + ' minutes'
                    });
                } else {
                    health.activeAgents++;
                    health.details.push({
                        agentId,
                        status: 'active',
                        age: Math.round(age / 1000 / 60) + ' minutes'
                    });
                }
            } catch (error) {
                health.corruptedStates++;
                health.details.push({
                    agentId,
                    status: 'corrupted',
                    error: error.message
                });
            }
        });

        return health;
    }

    cleanup(maxAge = 60 * 60 * 1000) {
        const stateFiles = fs.readdirSync(this.stateDir)
            .filter(file => file.endsWith('.json'));

        let cleanedUp = 0;

        stateFiles.forEach(file => {
            const statePath = path.join(this.stateDir, file);
            const stats = fs.statSync(statePath);
            const age = Date.now() - stats.mtime.getTime();

            if (age > maxAge) {
                fs.unlinkSync(statePath);
                cleanedUp++;
            }
        });

        return cleanedUp;
    }
}

module.exports = AgentRecovery;
EOF

echo "‚úÖ Agent recovery system created"
```

---

## üìä Monitoring and Alerting Setup

### Comprehensive Health Dashboard

**Health Monitor:**
```bash
# Create comprehensive health monitor
cat > bmad-method/utils/health-monitor.js << 'EOF'
const fs = require('fs');
const { spawn } = require('child_process');
const ResourceMonitor = require('./resource-monitor');
const AgentRecovery = require('./agent-recovery');

class HealthMonitor {
    constructor() {
        this.resourceMonitor = new ResourceMonitor();
        this.agentRecovery = new AgentRecovery();
        this.healthChecks = [];
        this.isRunning = false;
    }

    async startMonitoring() {
        if (this.isRunning) return;
        
        this.isRunning = true;
        this.resourceMonitor.start_monitoring();
        
        // Run health checks every 30 seconds
        this.healthCheckInterval = setInterval(() => {
            this.runHealthChecks();
        }, 30000);

        console.log('üöÄ Health monitoring started');
    }

    stopMonitoring() {
        this.isRunning = false;
        
        if (this.healthCheckInterval) {
            clearInterval(this.healthCheckInterval);
        }
        
        this.resourceMonitor.stop_monitoring();
        console.log('üõë Health monitoring stopped');
    }

    async runHealthChecks() {
        const results = {
            timestamp: new Date(),
            overall_status: 'HEALTHY',
            checks: {}
        };

        // System resource check
        results.checks.resources = await this.checkSystemResources();
        
        // Database connectivity check
        results.checks.database = await this.checkDatabaseConnectivity();
        
        // Agent state check
        results.checks.agents = this.checkAgentStates();
        
        // Python environment check
        results.checks.python = await this.checkPythonEnvironment();
        
        // Disk space check
        results.checks.disk = await this.checkDiskSpace();

        // Determine overall status
        const statuses = Object.values(results.checks).map(check => check.status);
        if (statuses.includes('CRITICAL')) {
            results.overall_status = 'CRITICAL';
        } else if (statuses.includes('WARNING')) {
            results.overall_status = 'WARNING';
        }

        this.healthChecks.push(results);
        
        // Keep only last 100 health checks
        if (this.healthChecks.length > 100) {
            this.healthChecks = this.healthChecks.slice(-100);
        }

        // Log critical issues
        if (results.overall_status === 'CRITICAL') {
            console.error('üö® CRITICAL health issues detected:', results);
        }

        return results;
    }

    async checkSystemResources() {
        try {
            const metrics = this.resourceMonitor.collect_metrics();
            
            let status = 'HEALTHY';
            const issues = [];

            if (metrics.cpu && metrics.cpu.percent > 90) {
                status = 'CRITICAL';
                issues.push(`CPU usage: ${metrics.cpu.percent}%`);
            } else if (metrics.cpu && metrics.cpu.percent > 75) {
                status = 'WARNING';
                issues.push(`CPU usage: ${metrics.cpu.percent}%`);
            }

            if (metrics.memory && metrics.memory.percent > 95) {
                status = 'CRITICAL';
                issues.push(`Memory usage: ${metrics.memory.percent}%`);
            } else if (metrics.memory && metrics.memory.percent > 85) {
                status = 'WARNING';
                issues.push(`Memory usage: ${metrics.memory.percent}%`);
            }

            return {
                status,
                metrics,
                issues,
                message: issues.length ? issues.join(', ') : 'Resources healthy'
            };
        } catch (error) {
            return {
                status: 'CRITICAL',
                error: error.message,
                message: 'Failed to check system resources'
            };
        }
    }

    async checkDatabaseConnectivity() {
        return new Promise((resolve) => {
            const python = spawn('python', ['-c', `
import duckdb
try:
    conn = duckdb.connect('data/analytics.duckdb')
    conn.execute('SELECT 1').fetchone()
    conn.close()
    print('HEALTHY')
except Exception as e:
    print(f'CRITICAL: {e}')
            `]);

            let output = '';
            python.stdout.on('data', (data) => output += data.toString());
            python.stderr.on('data', (data) => output += data.toString());

            python.on('close', (code) => {
                if (output.includes('HEALTHY')) {
                    resolve({
                        status: 'HEALTHY',
                        message: 'Database connectivity OK'
                    });
                } else {
                    resolve({
                        status: 'CRITICAL',
                        message: 'Database connectivity failed',
                        error: output.trim()
                    });
                }
            });

            // Timeout after 10 seconds
            setTimeout(() => {
                python.kill();
                resolve({
                    status: 'CRITICAL',
                    message: 'Database connectivity check timeout'
                });
            }, 10000);
        });
    }

    checkAgentStates() {
        try {
            const agentHealth = this.agentRecovery.healthCheck();
            
            let status = 'HEALTHY';
            if (agentHealth.corruptedStates > 0) {
                status = 'CRITICAL';
            } else if (agentHealth.staleAgents > agentHealth.activeAgents) {
                status = 'WARNING';
            }

            return {
                status,
                message: `${agentHealth.activeAgents} active, ${agentHealth.staleAgents} stale, ${agentHealth.corruptedStates} corrupted`,
                details: agentHealth
            };
        } catch (error) {
            return {
                status: 'CRITICAL',
                error: error.message,
                message: 'Failed to check agent states'
            };
        }
    }

    async checkPythonEnvironment() {
        return new Promise((resolve) => {
            const python = spawn('python', ['-c', `
import sys
import pkg_resources
required_packages = ['duckdb', 'sqlmesh', 'dagster', 'pyairbyte']
missing = []
for package in required_packages:
    try:
        pkg_resources.get_distribution(package)
    except:
        missing.append(package)

if missing:
    print(f'WARNING: Missing packages: {missing}')
else:
    print('HEALTHY')
            `]);

            let output = '';
            python.stdout.on('data', (data) => output += data.toString());
            python.stderr.on('data', (data) => output += data.toString());

            python.on('close', (code) => {
                if (output.includes('HEALTHY')) {
                    resolve({
                        status: 'HEALTHY',
                        message: 'Python environment OK'
                    });
                } else if (output.includes('WARNING')) {
                    resolve({
                        status: 'WARNING',
                        message: output.trim()
                    });
                } else {
                    resolve({
                        status: 'CRITICAL',
                        message: 'Python environment check failed',
                        error: output.trim()
                    });
                }
            });

            setTimeout(() => {
                python.kill();
                resolve({
                    status: 'CRITICAL',
                    message: 'Python environment check timeout'
                });
            }, 10000);
        });
    }

    async checkDiskSpace() {
        try {
            const stats = fs.statSync('.');
            // This is a simplified check - in production you'd want more sophisticated disk space monitoring
            
            return {
                status: 'HEALTHY',
                message: 'Disk space check passed'
            };
        } catch (error) {
            return {
                status: 'WARNING',
                message: 'Could not check disk space',
                error: error.message
            };
        }
    }

    getHealthSummary() {
        if (this.healthChecks.length === 0) {
            return { status: 'UNKNOWN', message: 'No health checks performed yet' };
        }

        const latest = this.healthChecks[this.healthChecks.length - 1];
        return {
            status: latest.overall_status,
            timestamp: latest.timestamp,
            summary: latest.checks,
            history_count: this.healthChecks.length
        };
    }

    generateAlert(severity, message, details = null) {
        const alert = {
            severity,
            message,
            details,
            timestamp: new Date(),
            id: Math.random().toString(36).substr(2, 9)
        };

        console.log(`üö® ALERT [${severity}]: ${message}`);
        if (details) {
            console.log('Details:', JSON.stringify(details, null, 2));
        }

        return alert;
    }
}

module.exports = HealthMonitor;
EOF

echo "‚úÖ Health monitor created"
```

---

## üéØ Quick Resolution Checklist

### Common Issue Resolution Steps

#### **P0 - Critical Issues**

**Agent Won't Activate:**
```bash
# 1. Check expansion pack installation
npx bmad-method list-agents | grep "data-"

# 2. Verify agent file exists
ls -la bmad-method/expansion-packs/bmad-data-practitioner/agents/

# 3. Test Node.js + Python communication
node -e "const { spawn } = require('child_process'); spawn('python', ['--version']).stdout.on('data', d => console.log(d.toString()))"

# 4. Reinstall if needed
npm run install:bmad
```

**Database Completely Inaccessible:**
```bash
# 1. Check file permissions
ls -la data/analytics.duckdb

# 2. Kill any hanging processes
ps aux | grep python | grep duckdb
kill -9 [process_ids]

# 3. Remove lock files
rm -f data/analytics.duckdb.wal data/analytics.duckdb-shm

# 4. Test basic connectivity
python -c "import duckdb; conn = duckdb.connect(':memory:'); print('‚úÖ DuckDB working')"
```

#### **P1 - High Priority Issues**

**Slow Query Performance:**
```bash
# 1. Profile specific queries
python bmad-method/utils/query-profiler.py

# 2. Check memory configuration
python -c "import duckdb; conn = duckdb.connect('data/analytics.duckdb'); conn.execute('PRAGMA memory_limit = \"1GB\"'); conn.execute('SET threads = 2')"

# 3. Analyze query plans
# Add EXPLAIN to queries for optimization
```

**Memory Leaks:**
```bash
# 1. Start resource monitoring
python bmad-method/utils/resource-monitor.py

# 2. Check for orphaned processes
ps aux | grep python

# 3. Implement connection pooling
# Use DuckDBConnectionManager from examples above
```

#### **P2 - Medium Priority Issues**

**Configuration Conflicts:**
```bash
# 1. Validate configuration
node bmad-method/utils/config-validator.js

# 2. Check engine conflicts
node bmad-method/utils/engine-detector.js

# 3. Reset to defaults if needed
cp config/environments/local.yaml.template config/environments/local.yaml
```

#### **P3 - Low Priority Issues**

**Documentation Updates:**
```bash
# 1. Update version numbers
# 2. Refresh examples with current data
# 3. Validate all links and references
```

---

## üìö Extended Diagnostics Resources

### Advanced Debugging Commands

**Complete System Diagnostic:**
```bash
# Create comprehensive diagnostic script
cat > bmad-method/utils/system-diagnostic.sh << 'EOF'
#!/bin/bash

echo "üîç BMad Data Practitioner System Diagnostic"
echo "=========================================="

echo "üìã Environment Information:"
echo "  Node.js: $(node --version)"
echo "  Python: $(python --version)"
echo "  npm: $(npm --version)"
echo "  Platform: $(uname -s)"

echo "üì¶ Package Status:"
npm list --depth=0 | grep bmad
pip list | grep -E "(duckdb|sqlmesh|dagster|pyairbyte)"

echo "üóÑÔ∏è Database Status:"
if [ -f "data/analytics.duckdb" ]; then
    echo "  ‚úÖ Database file exists ($(du -h data/analytics.duckdb | cut -f1))"
    python -c "import duckdb; conn = duckdb.connect('data/analytics.duckdb'); print('  ‚úÖ Database accessible'); conn.close()" 2>/dev/null || echo "  ‚ùå Database connection failed"
else
    echo "  ‚ùå Database file not found"
fi

echo "ü§ñ Agent Status:"
for agent in data-engineer data-analyst data-architect data-product-manager ml-engineer data-qa-engineer; do
    if [ -f "bmad-method/expansion-packs/bmad-data-practitioner/agents/${agent}.md" ]; then
        echo "  ‚úÖ ${agent} available"
    else
        echo "  ‚ùå ${agent} missing"
    fi
done

echo "üíæ Resource Usage:"
echo "  Memory: $(free -m | awk 'NR==2{printf "%.1f%%", $3*100/$2 }')"
echo "  Disk: $(df -h . | awk 'NR==2{print $5 " used"}')"

echo "üîß Process Check:"
echo "  Python processes: $(ps aux | grep python | wc -l)"
echo "  Node processes: $(ps aux | grep node | wc -l)"

echo "üìä Recent Errors:"
if [ -f "logs/error.log" ]; then
    tail -n 5 logs/error.log
else
    echo "  No error log found"
fi

echo "‚úÖ Diagnostic complete"
EOF

chmod +x bmad-method/utils/system-diagnostic.sh
echo "‚úÖ System diagnostic script created"
```

**Performance Benchmarking:**
```bash
# Create performance benchmark suite
cat > bmad-method/utils/performance-benchmark.py << 'EOF'
import time
import duckdb
import json
from typing import Dict, List

class PerformanceBenchmark:
    def __init__(self):
        self.results = []
    
    def benchmark_database_operations(self) -> Dict:
        """Benchmark basic database operations"""
        print("üöÄ Running database performance benchmark...")
        
        benchmark_results = {
            'connection_time': self._benchmark_connection(),
            'table_creation': self._benchmark_table_creation(),
            'data_insertion': self._benchmark_data_insertion(),
            'query_performance': self._benchmark_queries()
        }
        
        return benchmark_results
    
    def _benchmark_connection(self) -> float:
        """Benchmark database connection time"""
        start_time = time.time()
        conn = duckdb.connect(':memory:')
        conn.close()
        return time.time() - start_time
    
    def _benchmark_table_creation(self) -> float:
        """Benchmark table creation time"""
        conn = duckdb.connect(':memory:')
        
        start_time = time.time()
        conn.execute("""
            CREATE TABLE benchmark_test (
                id INTEGER,
                name VARCHAR,
                value DECIMAL(10,2),
                created_at TIMESTAMP
            )
        """)
        end_time = time.time()
        
        conn.close()
        return end_time - start_time
    
    def _benchmark_data_insertion(self) -> Dict:
        """Benchmark data insertion performance"""
        conn = duckdb.connect(':memory:')
        
        # Create test table
        conn.execute("""
            CREATE TABLE benchmark_insert (
                id INTEGER,
                data VARCHAR,
                value DECIMAL(10,2)
            )
        """)
        
        # Benchmark single row insert
        start_time = time.time()
        conn.execute("INSERT INTO benchmark_insert VALUES (1, 'test', 123.45)")
        single_row_time = time.time() - start_time
        
        # Benchmark bulk insert
        start_time = time.time()
        for i in range(1000):
            conn.execute(f"INSERT INTO benchmark_insert VALUES ({i}, 'test_{i}', {i * 1.5})")
        bulk_insert_time = time.time() - start_time
        
        conn.close()
        
        return {
            'single_row': single_row_time,
            'bulk_1000_rows': bulk_insert_time
        }
    
    def _benchmark_queries(self) -> Dict:
        """Benchmark query performance"""
        conn = duckdb.connect(':memory:')
        
        # Create and populate test table
        conn.execute("""
            CREATE TABLE benchmark_query AS 
            SELECT 
                range as id,
                'customer_' || range as name,
                random() * 1000 as value,
                current_timestamp as created_at
            FROM range(10000)
        """)
        
        queries = {
            'count': "SELECT COUNT(*) FROM benchmark_query",
            'simple_filter': "SELECT * FROM benchmark_query WHERE id < 100",
            'aggregation': "SELECT AVG(value), SUM(value), MAX(value) FROM benchmark_query",
            'group_by': "SELECT name, COUNT(*) FROM benchmark_query GROUP BY name LIMIT 10"
        }
        
        query_times = {}
        
        for query_name, query in queries.items():
            start_time = time.time()
            result = conn.execute(query).fetchall()
            query_times[query_name] = {
                'time': time.time() - start_time,
                'rows_returned': len(result)
            }
        
        conn.close()
        return query_times
    
    def generate_report(self, benchmark_results: Dict) -> str:
        """Generate human-readable benchmark report"""
        report = "\nüìä Performance Benchmark Report\n"
        report += "=" * 40 + "\n\n"
        
        # Connection performance
        conn_time = benchmark_results['connection_time'] * 1000
        report += f"üîå Connection Time: {conn_time:.2f}ms\n"
        
        # Table creation
        table_time = benchmark_results['table_creation'] * 1000
        report += f"üèóÔ∏è  Table Creation: {table_time:.2f}ms\n"
        
        # Data insertion
        insert = benchmark_results['data_insertion']
        report += f"üìù Single Row Insert: {insert['single_row'] * 1000:.2f}ms\n"
        report += f"üì¶ Bulk Insert (1K rows): {insert['bulk_1000_rows']:.2f}s\n"
        
        # Query performance
        report += "\nüîç Query Performance:\n"
        for query_name, metrics in benchmark_results['query_performance'].items():
            time_ms = metrics['time'] * 1000
            rows = metrics['rows_returned']
            report += f"  {query_name}: {time_ms:.2f}ms ({rows} rows)\n"
        
        # Performance assessment
        report += "\nüìà Performance Assessment:\n"
        
        if conn_time < 50:
            report += "  ‚úÖ Connection time: Excellent\n"
        elif conn_time < 100:
            report += "  ‚ö†Ô∏è  Connection time: Good\n"
        else:
            report += "  ‚ùå Connection time: Needs improvement\n"
        
        total_query_time = sum(m['time'] for m in benchmark_results['query_performance'].values())
        if total_query_time < 0.1:
            report += "  ‚úÖ Query performance: Excellent\n"
        elif total_query_time < 0.5:
            report += "  ‚ö†Ô∏è  Query performance: Good\n"
        else:
            report += "  ‚ùå Query performance: Needs optimization\n"
        
        return report

if __name__ == "__main__":
    benchmark = PerformanceBenchmark()
    results = benchmark.benchmark_database_operations()
    
    print(benchmark.generate_report(results))
    
    # Save detailed results
    with open('benchmark_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nüíæ Detailed results saved to benchmark_results.json")
EOF

echo "‚úÖ Performance benchmark suite created"
```

This comprehensive troubleshooting guide provides data practitioners with intelligent error recognition, cross-language debugging techniques, performance optimization tools, and automated recovery procedures. The modular utilities can be customized for specific environments and integrated into existing monitoring workflows.

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "Create Priority 1 Deliverable 1: Complete Onboarding Masterclass", "status": "completed", "id": "1"}, {"content": "Create Priority 1 Deliverable 2: Role-Based Daily Workflows", "status": "completed", "id": "2"}, {"content": "Create Priority 1 Deliverable 3: End-to-End Pipeline Masterclass", "status": "completed", "id": "3"}, {"content": "Create Priority 1 Deliverable 4: Advanced Troubleshooting Intelligence", "status": "completed", "id": "4"}, {"content": "Create Priority 1 Deliverable 5: Environment Setup Mastery", "status": "in_progress", "id": "5"}]