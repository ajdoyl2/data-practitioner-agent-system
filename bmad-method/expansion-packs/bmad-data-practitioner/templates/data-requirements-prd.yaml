template:
  id: data-requirements-prd-v1
  name: Data Requirements Product Requirements Document
  version: 1.0
  output:
    format: markdown
    filename: docs/data-requirements-prd.md
    title: "{{project_name}} Data Requirements PRD"

workflow:
  mode: interactive
  elicitation: advanced-elicitation

sections:
  - id: goals-context
    title: Data Goals and Business Context
    instruction: |
      Establish the business value and context for this data initiative. Focus on outcomes, stakeholders, and success metrics that align with business objectives.
    sections:
      - id: business-objectives
        title: Business Objectives
        type: bullet-list
        instruction: List the key business outcomes this data initiative will enable - focus on measurable impact
      - id: stakeholders
        title: Key Stakeholders
        type: bullet-list
        instruction: Identify all stakeholders who will use, benefit from, or be impacted by this data product
      - id: success-metrics
        title: Success Metrics
        type: bullet-list
        instruction: Define specific, measurable success criteria (KPIs, OKRs) for this data initiative
      - id: background-context
        title: Background Context
        type: paragraphs
        instruction: 1-2 paragraphs on current state, problems being solved, and why this data initiative is needed now
      - id: changelog
        title: Change Log
        type: table
        columns: [Date, Version, Description, Author]
        instruction: Track document versions and changes

  - id: data-requirements
    title: Data Requirements
    instruction: Define the complete data landscape requirements for this initiative
    elicit: true
    sections:
      - id: data-sources
        title: Data Sources
        type: numbered-list
        prefix: DS
        instruction: Identify all data sources needed - internal databases, external APIs, files, etc.
        examples:
          - "DS1: Customer transaction data from PostgreSQL production database (orders, payments, refunds)"
          - "DS2: Marketing campaign data from HubSpot API (campaigns, contacts, engagement metrics)"
      - id: data-outputs
        title: Data Outputs
        type: numbered-list
        prefix: DO
        instruction: Define the data products and outputs this initiative will produce
        examples:
          - "DO1: Daily customer behavior dashboard with key metrics and trends"
          - "DO2: Weekly executive report on marketing ROI and campaign performance"
      - id: data-quality
        title: Data Quality Requirements
        type: numbered-list
        prefix: DQ
        instruction: Specify data quality standards, validation rules, and accuracy requirements
        examples:
          - "DQ1: Customer email addresses must be validated and have >95% deliverability"
          - "DQ2: Financial data must have 100% accuracy with automated reconciliation"
      - id: data-governance
        title: Data Governance Requirements
        type: numbered-list
        prefix: DG
        instruction: Define data governance, privacy, security, and compliance requirements
        examples:
          - "DG1: All customer PII must be encrypted at rest and in transit"
          - "DG2: Data retention policies must comply with GDPR (7 years max for financial data)"

  - id: technical-requirements
    title: Technical Requirements
    instruction: Define the technical architecture and performance requirements for the data system
    elicit: true
    choices:
      data_volume: [Small (<1GB), Medium (1GB-1TB), Large (1TB-100TB), Very Large (>100TB)]
      processing_frequency: [Real-time, Hourly, Daily, Weekly, Monthly, Ad-hoc]
      availability: [Standard (99%), High (99.9%), Critical (99.99%)]
    sections:
      - id: performance-requirements
        title: Performance Requirements
        type: numbered-list
        prefix: PR
        instruction: Specify performance, scalability, and latency requirements
        examples:
          - "PR1: Dashboard queries must return results within 3 seconds for datasets up to 10M rows"
          - "PR2: Data ingestion must process 100K records per hour during peak usage"
      - id: integration-requirements
        title: Integration Requirements
        type: numbered-list
        prefix: IR
        instruction: Define system integrations, APIs, and data connectivity requirements
        examples:
          - "IR1: Real-time integration with Salesforce CRM via REST API"
          - "IR2: Batch integration with data warehouse via secure SFTP"
      - id: technology-stack
        title: Technology Stack Requirements
        instruction: Document preferred technologies, tools, and constraints
        sections:
          - id: data_volume
            title: "Data Volume: {Small (<1GB)|Medium (1GB-1TB)|Large (1TB-100TB)|Very Large (>100TB)}"
          - id: processing_frequency
            title: "Processing Frequency: {Real-time|Hourly|Daily|Weekly|Monthly|Ad-hoc}"
          - id: availability_requirements
            title: "Availability Requirements: {Standard (99%)|High (99.9%)|Critical (99.99%)}"
          - id: preferred_technologies
            title: Preferred Technologies
            instruction: List any required or preferred data technologies, tools, or platforms

  - id: user-personas
    title: Data User Personas
    instruction: Define the different types of users who will interact with this data system
    elicit: true
    sections:
      - id: primary-users
        title: Primary Users
        type: bullet-list
        instruction: Identify the main users who will directly use the data products
      - id: secondary-users
        title: Secondary Users
        type: bullet-list
        instruction: Identify users who will benefit from or be impacted by the data products
      - id: user-workflows
        title: Key User Workflows
        type: numbered-list
        instruction: Describe the main workflows users will follow when using the data products
        examples:
          - "Marketing analyst reviews daily dashboard to identify campaign performance trends"
          - "Executive accesses weekly report during leadership meetings for strategic decisions"

  - id: data-pipeline-requirements
    title: Data Pipeline Requirements
    instruction: Define the data processing pipeline and workflow requirements
    elicit: true
    sections:
      - id: ingestion-requirements
        title: Data Ingestion Requirements
        type: bullet-list
        instruction: Specify how data will be collected, ingested, and initially processed
      - id: transformation-requirements
        title: Data Transformation Requirements
        type: bullet-list
        instruction: Define data cleansing, enrichment, and transformation needs
      - id: storage-requirements
        title: Data Storage Requirements
        type: bullet-list
        instruction: Specify data storage, retention, and archival requirements
      - id: delivery-requirements
        title: Data Delivery Requirements
        type: bullet-list
        instruction: Define how processed data will be delivered to users and systems

  - id: analytics-requirements
    title: Analytics and Insights Requirements
    instruction: Define the analytical capabilities and insights needed from the data
    elicit: true
    sections:
      - id: reporting-requirements
        title: Reporting Requirements
        type: numbered-list
        instruction: Specify standard reports, dashboards, and visualizations needed
        examples:
          - "Monthly customer acquisition cost (CAC) report with trend analysis"
          - "Real-time sales performance dashboard with geographic breakdown"
      - id: analysis-requirements
        title: Analysis Requirements
        type: numbered-list
        instruction: Define analytical investigations, statistical analysis, and data science needs
        examples:
          - "Customer segmentation analysis to identify high-value customer groups"
          - "Predictive analysis for sales forecasting and inventory optimization"
      - id: insight-requirements
        title: Insight Requirements
        type: numbered-list
        instruction: Specify the business insights and actionable recommendations expected
        examples:
          - "Automated alerts when key metrics deviate from expected ranges"
          - "Recommendations for marketing campaign optimization based on performance data"

  - id: epic-list
    title: Data Initiative Epic List
    instruction: |
      Present a high-level list of all epics for this data initiative. Each epic should deliver significant functionality in the data pipeline.
      
      CRITICAL: Epics MUST follow data pipeline logical sequence:
      
      - Epic 1: Data Foundation & Infrastructure (agent setup, basic ingestion, core analytics)
      - Epic 2: Data Processing & Transformation (ETL/ELT workflows, data quality)
      - Epic 3: Analytics & Intelligence (automated analysis, insights generation)
      - Epic 4: Delivery & Consumption (dashboards, reports, API access)
      
      Each epic should deliver deployable, testable functionality that provides value.
    elicit: true
    examples:
      - "Epic 1: Data Foundation: Establish data infrastructure, ingestion pipeline, and core analytics capability"
      - "Epic 2: Data Processing: Implement transformation workflows, data quality framework, and automated processing"
      - "Epic 3: Analytics Engine: Deploy automated analysis, statistical testing, and insight generation"
      - "Epic 4: Insight Delivery: Create dashboards, reports, and publication-quality outputs"

  - id: epic-details
    title: Epic {{epic_number}} {{epic_title}}
    repeatable: true
    instruction: |
      Present each data epic with complete story breakdown and acceptance criteria.
      
      For each epic, provide expanded goal describing the data capability and business value.
      
      CRITICAL DATA STORY REQUIREMENTS:
      
      - Stories must follow data pipeline sequence (ingestion → processing → analysis → delivery)
      - Each story must deliver testable data functionality
      - Stories should be sized for focused AI agent execution (2-4 hours of work)
      - Include data quality validation in every processing story
      - Ensure data governance and security requirements are built into relevant stories
    elicit: true
    template: "{{epic_goal}}"
    sections:
      - id: story
        title: Story {{epic_number}}.{{story_number}} {{story_title}}
        repeatable: true
        template: |
          As a {{user_type}},
          I want {{action}},
          so that {{benefit}}.
        sections:
          - id: acceptance-criteria
            title: Acceptance Criteria
            type: numbered-list
            item_template: "{{criterion_number}}: {{criteria}}"
            repeatable: true
            instruction: |
              Define data-specific acceptance criteria including:
              
              - Data quality validation and testing requirements
              - Performance benchmarks for data processing
              - Security and governance compliance checks
              - Integration verification with upstream/downstream systems
              - User acceptance criteria for data consumers

  - id: checklist-results
    title: Checklist Results Report
    instruction: Execute the data-product-checklist to validate completeness and quality of this PRD

  - id: next-steps
    title: Next Steps
    sections:
      - id: data-architect-prompt
        title: Data Architect Prompt
        instruction: Prompt for the Data Architect to create technical architecture based on this PRD
      - id: data-engineer-prompt
        title: Data Engineer Prompt
        instruction: Prompt for the Data Engineer to begin implementation planning